{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Building in XGBoost\n",
    "\n",
    "This is a great article for tunning XGboost: http://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "windows=True\n",
    "if (windows):\n",
    "    mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-5.3.0-posix-seh-rt_v4-rev0\\\\mingw64\\\\bin'\n",
    "    os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "import math\n",
    "\n",
    "_start_time = time.time()\n",
    "\n",
    "def tic():\n",
    "    global _start_time \n",
    "    _start_time = time.time()\n",
    "\n",
    "def tac():\n",
    "    t_sec = round(time.time() - _start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60) \n",
    "    print('Time passed: {}hour:{}min:{}sec'.format(t_hour,t_min,t_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed: 0hour:1min:34sec\n"
     ]
    }
   ],
   "source": [
    "#now we load our modified train and test set\n",
    "\n",
    "tic()\n",
    "#train = pd.read_csv('./input-data/train_modified.csv', nrows = 500000)\n",
    "#test = pd.read_csv('./input-data/test_modified.csv', nrows = 500000)\n",
    "train = pd.read_csv('./input-data/train_modified_5lags.csv',\n",
    "                    dtype = {'Semana': 'int8',\n",
    "                            'Agencia_ID': 'uint16',\n",
    "                            'Canal_ID': 'int8',\n",
    "                            'Ruta_SAK': 'int32',\n",
    "                            'Cliente_ID': 'int32',\n",
    "                            'Producto_ID': 'int32',\n",
    "                            'log_target':  'float64',\n",
    "                            'Log_Target_mean_lag1': 'float64',\n",
    "                            'Log_Target_mean_lag2': 'float64',\n",
    "                            'Log_Target_mean_lag3': 'float64',\n",
    "                            'Log_Target_mean_lag4': 'float64',\n",
    "                            'Log_Target_mean_lag5': 'float64',\n",
    "                            'rate_change2': 'float64',\n",
    "                            'rate_change3': 'float64',\n",
    "                            'rate_change4': 'float64',\n",
    "                            'Lags_sum': 'float64',\n",
    "                            'pairs_mean':  'float64',\n",
    "                            'brand': 'uint16',\n",
    "                            'cluster': 'uint16',\n",
    "                            'Qty_Ruta_SAK_Bin': 'int8',\n",
    "                            'ZipCode': 'uint16',\n",
    "                            'week_ct': 'int8',\n",
    "                            'NombreCliente': 'int8',\n",
    "                            'Producto_ID_clust_ID':'int8',\n",
    "                            'Ruta_SAK_clust_ID':'int8',\n",
    "                            'Agencia_ID_clust_ID':'int8'},\n",
    "                   )\n",
    "                    \n",
    "val = pd.read_csv('./input-data/val_modified_5lags.csv',\n",
    "                    dtype = {'Semana': 'int8',\n",
    "                            'Agencia_ID': 'uint16',\n",
    "                            'Canal_ID': 'int8',\n",
    "                            'Ruta_SAK': 'int32',\n",
    "                            'Cliente_ID': 'int32',\n",
    "                            'Producto_ID': 'int32',\n",
    "                            'log_target':  'float64',\n",
    "                            'Log_Target_mean_lag1': 'float64',\n",
    "                            'Log_Target_mean_lag2': 'float64',\n",
    "                            'Log_Target_mean_lag3': 'float64',\n",
    "                            'Log_Target_mean_lag4': 'float64',\n",
    "                            'Log_Target_mean_lag5': 'float64',\n",
    "                            'rate_change2': 'float64',\n",
    "                            'rate_change3': 'float64',\n",
    "                            'rate_change4': 'float64',\n",
    "                            'Lags_sum': 'float64',\n",
    "                            'pairs_mean':  'float64',\n",
    "                            'brand': 'uint16',\n",
    "                            'cluster': 'uint16',\n",
    "                            'Qty_Ruta_SAK_Bin': 'int8',\n",
    "                            'ZipCode': 'uint16',\n",
    "                            'week_ct': 'int8',\n",
    "                            'NombreCliente': 'int8',\n",
    "                            'Producto_ID_clust_ID':'int8',\n",
    "                            'Ruta_SAK_clust_ID':'int8',\n",
    "                            'Agencia_ID_clust_ID':'int8'},\n",
    "                   ) \n",
    "    \n",
    "test = pd.read_csv('./input-data/test_modified_5lags.csv',\n",
    "                    dtype = {'id': 'uint32',\n",
    "                            'Semana': 'int8',\n",
    "                            'Agencia_ID': 'uint16',\n",
    "                            'Canal_ID': 'int8',\n",
    "                            'Ruta_SAK': 'int32',\n",
    "                            'Cliente_ID': 'int32',\n",
    "                            'Producto_ID': 'int32',\n",
    "                            'Log_Target_mean_lag1': 'float64',\n",
    "                            'Log_Target_mean_lag2': 'float64',\n",
    "                            'Log_Target_mean_lag3': 'float64',\n",
    "                            'Log_Target_mean_lag4': 'float64',\n",
    "                            'Log_Target_mean_lag5': 'float64',\n",
    "                            'rate_change2': 'float64',\n",
    "                            'rate_change3': 'float64',\n",
    "                            'rate_change4': 'float64',\n",
    "                            'Lags_sum': 'float64',\n",
    "                            'pairs_mean': 'float64',\n",
    "                            'brand': 'uint16',\n",
    "                            'cluster': 'uint16',\n",
    "                            'Qty_Ruta_SAK_Bin': 'int8',\n",
    "                            'ZipCode': 'uint16',\n",
    "                            'week_ct': 'int8',\n",
    "                            'NombreCliente': 'int8',\n",
    "                            'Producto_ID_clust_ID':'int8',\n",
    "                            'Ruta_SAK_clust_ID':'int8',\n",
    "                            'Agencia_ID_clust_ID':'int8'},\n",
    "                      )\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define target and ID columns:\n",
    "target = 'log_target'\n",
    "IDcol = 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "def modelfit(alg, dtrain, dval, dtest, predictors, target, IDcol, filename):\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    watchlist = [(dval[predictors], dval[target])]\n",
    "    alg.fit(dtrain[predictors], dtrain[target], eval_set=watchlist, eval_metric='rmse', early_stopping_rounds=10, verbose=False)\n",
    "    \n",
    "    alg.evals_result()\n",
    "\n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    \n",
    "    #Predict validation (holdout) set:\n",
    "    dval_predictions = alg.predict(dval[predictors])\n",
    "    dval_predictions = np.maximum(dval_predictions, 0)# we make all negative numbers = 0 since there cannot be a negative demand\n",
    "    \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print ('RMSLE TRAIN: ', np.sqrt(metrics.mean_squared_error(dtrain[target].values, dtrain_predictions)))\n",
    "    print ('RMSLE VAL: ', np.sqrt(metrics.mean_squared_error(dval[target].values, dval_predictions)))\n",
    "    \n",
    "        \n",
    "    #Predict on testing data: we need to revert it back to target by applying expm1\n",
    "    dtest[target] = np.expm1(alg.predict(dtest[predictors]))\n",
    "    \n",
    "    print ('NUM ROWS PREDICTED: ', dtest.shape[0] )\n",
    "    print ('NUM NEGATIVES PREDICTED: ', dtest[target][dtest[target] < 0].count())\n",
    "    print ('MIN TARGET PREDICTED: ', dtest[target].min())\n",
    "    print ('MEAN TARGET PREDICTED: ', dtest[target].mean())\n",
    "    print ('MAX TARGET PREDICTED: ', dtest[target].max())\n",
    "    \n",
    "    dtest[target] = np.maximum(dtest[target], 0) # we make all negative numbers = 0 since there cannot be a negative demand\n",
    "    \n",
    "    #Export submission file:\n",
    "    #IDcol.append(target)\n",
    "    #submission = pd.DataFrame({ x: dtest[x] for x in IDcol})\n",
    "    submission = dtest[[IDcol,target]].copy()\n",
    "    submission[IDcol] = submission[IDcol].astype(int)\n",
    "    submission.rename(columns={target: 'Demanda_uni_equil'}, inplace=True)\n",
    "    submission.to_csv(\"./Submissions/\"+filename, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alg10 - XGB-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the behavior of the sckit-learn models, we should remove pairs_mean feature to get a better prediction. We hence take all other features except Semana and pairs_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = ['Log_Target_mean_lag1',\n",
    "              'Log_Target_mean_lag2','Log_Target_mean_lag3','Log_Target_mean_lag4', 'Log_Target_mean_lag5',\n",
    "              'Lags_sum','brand','cluster','Qty_Ruta_SAK_Bin','ZipCode',\n",
    "              'week_ct','NombreCliente','Producto_ID_clust_ID','Ruta_SAK_clust_ID','Agencia_ID_clust_ID']\n",
    "\n",
    "model = xgb.XGBRegressor(n_estimators = 50, objective=\"reg:linear\", learning_rate= 0.1, max_depth=10,\n",
    "                         subsample=0.5)\n",
    "\n",
    "tic()\n",
    "modelfit(model, train, val, test, predictors, target, IDcol, 'alg10.csv')\n",
    "tac()\n",
    "\n",
    "feat_imp = pd.Series(model.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --> LB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great improvement from past algos. The most important thing that I see here is that the Feature importance map\n",
    "is very different from the H2O models. LB Scores between XGB and H2O are similar, so this is a great case for ensembling!\n",
    "\n",
    "Let's try more estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alg11 - XGB-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = ['Log_Target_mean_lag1',\n",
    "              'Log_Target_mean_lag2','Log_Target_mean_lag3','Log_Target_mean_lag4', 'Log_Target_mean_lag5',\n",
    "              'Lags_sum','brand','cluster','Qty_Ruta_SAK_Bin','ZipCode',\n",
    "              'week_ct','NombreCliente','Producto_ID_clust_ID','Ruta_SAK_clust_ID','Agencia_ID_clust_ID']\n",
    "\n",
    "model = xgb.XGBRegressor(n_estimators = 1000, objective=\"reg:linear\", learning_rate= 0.1, max_depth=10,\n",
    "                         subsample=0.85,colsample_bytree=0.7)\n",
    "\n",
    "tic()\n",
    "modelfit(model, train, val, test, predictors, target, IDcol, 'alg11.csv')\n",
    "tac()\n",
    "\n",
    "feat_imp = pd.Series(model.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --> LB: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improved a little bit as expected."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
