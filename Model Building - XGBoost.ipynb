{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Building in XGBoost\n",
    "\n",
    "This is a great article for tunning XGboost: http://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "import math\n",
    "\n",
    "_start_time = time.time()\n",
    "\n",
    "def tic():\n",
    "    global _start_time \n",
    "    _start_time = time.time()\n",
    "\n",
    "def tac():\n",
    "    t_sec = round(time.time() - _start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60) \n",
    "    print('Time passed: {}hour:{}min:{}sec'.format(t_hour,t_min,t_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed: 0hour:0min:47sec\n"
     ]
    }
   ],
   "source": [
    "#now we load our modified train and test set\n",
    "\n",
    "tic()\n",
    "#train = pd.read_csv('./input-data/train_modified.csv', nrows = 500000)\n",
    "#test = pd.read_csv('./input-data/test_modified.csv', nrows = 500000)\n",
    "train = pd.read_csv('./input-data/train_modified_noW9.csv',\n",
    "                    dtype = {'Semana': 'int8',\n",
    "                            'Agencia_ID': 'uint16',\n",
    "                            'Canal_ID': 'int8',\n",
    "                            'Ruta_SAK': 'int32',\n",
    "                            'Cliente_ID': 'int32',\n",
    "                            'Producto_ID': 'int32',\n",
    "                            'log_target':  'float64',\n",
    "                            'Log_Target_mean_lag1': 'float64',\n",
    "                            'Log_Target_mean_lag2': 'float64',\n",
    "                            'Log_Target_mean_lag3': 'float64',\n",
    "                            'Log_Target_mean_lag4': 'float64',\n",
    "                            'Lags_sum': 'float64',\n",
    "                            'pairs_mean':  'float64',\n",
    "                            'brand': 'uint16',\n",
    "                            'cluster': 'uint16',\n",
    "                            'Qty_Ruta_SAK_Bin': 'int8',\n",
    "                            'ZipCode': 'uint16'},\n",
    "                   )\n",
    "                    \n",
    "test = pd.read_csv('./input-data/test_modified.csv',\n",
    "                    dtype = {'id': 'uint32',\n",
    "                            'Semana': 'int8',\n",
    "                            'Agencia_ID': 'uint16',\n",
    "                            'Canal_ID': 'int8',\n",
    "                            'Ruta_SAK': 'int32',\n",
    "                            'Cliente_ID': 'int32',\n",
    "                            'Producto_ID': 'int32',\n",
    "                            'Log_Target_mean_lag1': 'float64',\n",
    "                            'Log_Target_mean_lag2': 'float64',\n",
    "                            'Log_Target_mean_lag3': 'float64',\n",
    "                            'Log_Target_mean_lag4': 'float64',\n",
    "                            'Lags_sum': 'float64',\n",
    "                            'pairs_mean': 'float64',\n",
    "                            'brand': 'uint16',\n",
    "                            'cluster': 'uint16',\n",
    "                            'Qty_Ruta_SAK_Bin': 'int8',\n",
    "                            'ZipCode': 'uint16'},\n",
    "                      )\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed: 0hour:0min:2sec\n"
     ]
    }
   ],
   "source": [
    "# We split the train set to get a validation set\n",
    "tic()\n",
    "val = train[train.Semana > 8] # Weeks 9\n",
    "train = train[train.Semana <=8] # Weeks 7,8\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define target and ID columns:\n",
    "target = 'log_target'\n",
    "IDcol = 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "def modelfit(alg, dtrain, dval, dtest, predictors, target, IDcol, filename):\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    watchlist = [(dval[predictors], dval[target])]\n",
    "    alg.fit(dtrain[predictors], dtrain[target], eval_set=watchlist, eval_metric='rmse', early_stopping_rounds=10, verbose=True)\n",
    "    alg.evals_result()\n",
    "\n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    \n",
    "    #Predict validation (holdout) set:\n",
    "    dval_predictions = alg.predict(dval[predictors])\n",
    "\n",
    "\n",
    "    #Perform cross-validation:\n",
    "    #cv_score = cross_validation.cross_val_score(alg, dtrain[predictors], dtrain[target], cv=2, scoring='mean_squared_error')\n",
    "    #cv_score = np.sqrt(np.abs(cv_score))\n",
    "    \n",
    "    \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    #print (\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error(dtrain[target].values, dtrain_predictions)))\n",
    "    #print (\"CV Score : Mean - %.4g | Std - %.4g | Min - %.4g | Max - %.4g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "    \n",
    "    #RMSLE Option 3: the fastest one\n",
    "    print ('RMSLE TRAIN: ', np.sqrt(metrics.mean_squared_error(dtrain[target].values, dtrain_predictions)))\n",
    "    print ('RMSLE VAL: ', np.sqrt(metrics.mean_squared_error(dval[target].values, dval_predictions)))\n",
    "    \n",
    "        \n",
    "    #Predict on testing data: we need to revert it back to target by applying expm1\n",
    "    dtest[target] = np.expm1(alg.predict(dtest[predictors])) \n",
    "    \n",
    "    \n",
    "    print ('NUM ROWS PREDICTED: ', dtest.shape[0] )\n",
    "    print ('NUM NEGATIVES PREDICTED: ', dtest[target][dtest[target] < 0].count())\n",
    "    print ('MIN TARGET PREDICTED: ', dtest[target].min())\n",
    "    print ('MEAN TARGET PREDICTED: ', dtest[target].mean())\n",
    "    print ('MAX TARGET PREDICTED: ', dtest[target].max())\n",
    "    \n",
    "    #Export submission file:\n",
    "    #IDcol.append(target)\n",
    "    #submission = pd.DataFrame({ x: dtest[x] for x in IDcol})\n",
    "    submission = dtest[[IDcol,target]].copy()\n",
    "    submission[IDcol] = submission[IDcol].astype(int)\n",
    "    submission.rename(columns={target: 'Demanda_uni_equil'}, inplace=True)\n",
    "    submission.to_csv(\"./Submissions/\"+filename, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alg10 - XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = ['Agencia_ID','Canal_ID','Ruta_SAK','Cliente_ID','Producto_ID','Log_Target_mean_lag1',\n",
    "                'Log_Target_mean_lag2','Log_Target_mean_lag3','Log_Target_mean_lag4','Lags_sum','brand','cluster',\n",
    "                'Qty_Ruta_SAK_Bin','ZipCode']\n",
    "\n",
    "alg10 = xgb.XGBRegressor(n_estimators = 2, objective=\"reg:linear\", learning_rate= 0.1, max_depth=10,\n",
    "                         subsample=0.85,colsample_bytree=0.7)\n",
    "\n",
    "tic()\n",
    "modelfit(alg10, train, val, test, predictors, target, IDcol, 'alg10.csv')\n",
    "tac()\n",
    "\n",
    "feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #Grid search\n",
    "    clf = GridSearchCV(alg,\n",
    "                   {'max_depth': [2,4,6],\n",
    "                    'n_estimators': [50,100,200]}, verbose=1)\n",
    "    clf.fit(X,y)\n",
    "    print(clf.best_score_)\n",
    "    print(clf.best_params_)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
