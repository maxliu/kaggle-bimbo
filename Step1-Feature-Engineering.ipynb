{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "I will explore the problem in following stages:\n",
    "\n",
    "1.  **Hypothesis Generation** – understanding the problem better by brainstorming possible factors that can impact the outcome\n",
    "2.  **Data Exploration** – looking at categorical and continuous feature summaries and making inferences about the data.\n",
    "3.  **Data Cleaning** – imputing missing values in the data and checking for outliers\n",
    "4.  **Feature Engineering** – modifying existing variables and creating new ones for analysis\n",
    "5.  **Model Building** – making predictive models on the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hypothesis Generation\n",
    "\n",
    "This involves understanding the problem and making some hypothesis about what could potentially have a good impact on the outcome. This is done BEFORE looking at the data, and we end up creating a laundry list of the different analysis which we can potentially perform if data is available.\n",
    "\n",
    "### The Problem Statement\n",
    "\n",
    "Understanding the problem statement is the first and foremost step:\n",
    "\n",
    "> In this competition, you will forecast the demand of a product for a given week, at a particular store. The dataset you are given consists of 9 weeks of sales transactions in Mexico. Every week, there are delivery trucks that deliver products to the vendors. Each transaction consists of sales and returns. Returns are the products that are unsold and expired. The demand for a product in a certain week is defined as the sales this week subtracted by the return next week.\n",
    "\n",
    "So the idea is to find out the demand of a product (sales - returns) per client, and store which impacts the sales of a product. Let’s think about some of the analysis that can be done and come up with certain hypothesis.\n",
    "\n",
    "### The Hypotheses\n",
    "\n",
    "I came up with the following hypothesis while thinking about the problem. Since we’re talking about stores and products, lets make different sets for each.\n",
    "\n",
    "**Store/Client Level Hypotheses:**\n",
    "\n",
    "1.  **Town type:** Stores located in urban or Tier 1 towns should have higher sales because of the higher income levels of people there.\n",
    "2.  **Population Density:** Stores located in densely populated areas should have higher sales because of more demand.\n",
    "3.  **Store Capacity:** Stores which are very big in size should have higher sales as they act like one-stop-shops and people would prefer getting everything from one place\n",
    "4.  **Competitors:** Stores having similar establishments nearby should have less sales because of more competition.\n",
    "5.  **Marketing:** Stores which have a good marketing division should have higher sales as it will be able to attract customers through the right offers and advertising.\n",
    "6.  **Location:** Stores located within popular marketplaces should have higher sales because of better access to customers.\n",
    "7.  **Customer Behavior:** Stores keeping the right set of products to meet the local needs of customers will have higher sales.\n",
    "8.  **Ambiance:** Stores which are well-maintained and managed by polite and humble people are expected to have higher footfall and thus higher sales.\n",
    "9.  **Season:** Store should sell more after customer's pay day: after 15th or 30th of the month\n",
    "\n",
    "**Product Level Hypotheses:**\n",
    "\n",
    "1.  **Brand:** Branded products should have higher sales because of higher trust in the customer.\n",
    "2.  **Packaging:** Products with good packaging can attract customers and sell more.\n",
    "3.  **Utility:** Daily use products should have a higher tendency to sell as compared to the specific use products.\n",
    "4.  **Display Area:** Products which are given bigger shelves in the store are likely to catch attention first and sell more.\n",
    "5.  **Visibility in Store:** The location of product in a store will impact sales. Ones which are right at entrance will catch the eye of customer first rather than the ones in back.\n",
    "6.  **Advertising:** Better advertising of products in the store will should higher sales in most cases.\n",
    "7.  **Promotional Offers:** Products accompanied with attractive offers and discounts will sell more.\n",
    "\n",
    "\n",
    "Lets move on to the data exploration where we will have a look at the data in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\\. Data Exploration\n",
    "\n",
    "I’ll be performing some basic data exploration here and come up with some inferences about the data.\n",
    "\n",
    "The first step is to look at the data and try to identify the information which we hypothesized vs the available data. A comparison between the data dictionary on the competition page and out hypotheses is shown below:\n",
    "\n",
    "![Image of Variables vs Hypothesis](files/../input-data/Variables_vs_Hyphotesis.png)\n",
    "\n",
    "We can summarize the findings as:\n",
    "\n",
    "** 9 Features Hypothesized but not found in actual data. **\n",
    "\n",
    "** 5 Features Hypothesized as well as present in the data **\n",
    "\n",
    "** 3 Features present in the data but not hypothesized. **\n",
    "\n",
    "\n",
    "We find features which we hypothesized, but data doesn’t carry and vice versa. We should look for open source data to fill the gaps if possible. Let’s start by loading the required libraries and data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import boto # to download from AWS S3 buckets\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "_start_time = time.time()\n",
    "\n",
    "# define a easy timing function to use going forward\n",
    "def tic():\n",
    "    global _start_time \n",
    "    _start_time = time.time()\n",
    "\n",
    "def tac():\n",
    "    t_sec = round(time.time() - _start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60) \n",
    "    print('Time passed: {}hour:{}min:{}sec'.format(t_hour,t_min,t_sec))\n",
    "    \n",
    "# utility function- display large dataframes in an html iframe\n",
    "def df_display(df, lines=500):\n",
    "    txt = (\"<iframe \" +\n",
    "           \"srcdoc='\" + df.head(lines).to_html() + \"' \" +\n",
    "           \"width=1000 height=500>\" +\n",
    "           \"</iframe>\")\n",
    "\n",
    "    return IPython.display.HTML(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's define variables that will define the behaviour of the whole script\n",
    "s3_path = 'http://bbts-kaggle.s3.amazonaws.com/bimbo/Pablo/'\n",
    "use_validation=True # splits train data into train + val sets\n",
    "val_week_threshold = 9 # (possible values 8 or 9)  - weeks 3,4,5,6,7 are train, and week 8.9 are val\n",
    "trimmed = False # removes weeks which doesn't have all the lags. If False, fills empty lags with 0\n",
    "lag = 5  # shifted mean_demand up to \"lag\" weeks\n",
    "if (val_week_threshold == 8): lag = 4\n",
    "retrieve_saved_work = True # skips long jobs by retrieving already saved picked files after Feature 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-486d1dbe6c6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m                                      \u001b[1;34m'Dev_uni_proxima'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'int8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                                      \u001b[1;34m'Dev_proxima'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                                      'Demanda_uni_equil':'int32'})\n\u001b[0m\u001b[0;32m     15\u001b[0m test = pd.read_csv(s3_path +'test.csv',\n\u001b[0;32m     16\u001b[0m                            dtype  = {'Semana': 'int8',\n",
      "\u001b[1;32m/anaconda/envs/py35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/anaconda/envs/py35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m _parser_defaults = {\n",
      "\u001b[1;32m/anaconda/envs/py35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    813\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skip_footer not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 815\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/anaconda/envs/py35/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.read (pandas/parser.c:8748)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_low_memory (pandas/parser.c:9428)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser._concatenate_chunks (pandas/parser.c:25134)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Read files:\n",
    "tic()\n",
    "train = pd.read_csv(s3_path +'train.csv',\n",
    "                           dtype  = {'Semana': 'int8',\n",
    "                                     'Producto_ID':'int32',\n",
    "                                     'Cliente_ID':'int32',\n",
    "                                     'Agencia_ID':'uint16',\n",
    "                                     'Canal_ID':'int8',\n",
    "                                     'Ruta_SAK':'int32',\n",
    "                                     'Venta_hoy':'float32',\n",
    "                                     'Venta_uni_hoy': 'int8',\n",
    "                                     'Dev_uni_proxima':'int8',\n",
    "                                     'Dev_proxima':'float32',\n",
    "                                     'Demanda_uni_equil':'int32'})\n",
    "test = pd.read_csv(s3_path +'test.csv',\n",
    "                           dtype  = {'Semana': 'int8',\n",
    "                                     'Producto_ID':'int32',\n",
    "                                     'Cliente_ID':'int32',\n",
    "                                     'Agencia_ID':'uint16',\n",
    "                                     'Canal_ID':'int8',\n",
    "                                     'Ruta_SAK':'int32'})\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove unnecessary fields in training data\n",
    "train.drop(['Venta_uni_hoy', 'Venta_hoy','Dev_uni_proxima', 'Dev_proxima'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Since test dataframe is not the same as train dataframe, we make them equal by removing and adding columns\n",
    "train.insert(0, 'id', np.nan)\n",
    "test.insert(7, 'Demanda_uni_equil', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, probably one of the most important steps is to split the train set in train and validation. And never touch the validation set for any feature engineering, and to treat it just like the unseen test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (use_validation):\n",
    "    tic()\n",
    "    val = train[train.Semana >= val_week_threshold] # Weeks 8,9\n",
    "    train = train[train.Semana < val_week_threshold] # Weeks 3,4,5,6,7\n",
    "    tac()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to combine both train and test data sets into one, perform feature engineering and then divide them later again. This saves the trouble of performing the same steps twice on test and train. Lets combine them into a dataframe ‘data’ with a ‘source’ column specifying where each observation belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic()\n",
    "train['source']='train'\n",
    "if (use_validation): \n",
    "    val['source']='val'\n",
    "test['source']='test'\n",
    "\n",
    "if (use_validation): \n",
    "    data = pd.concat([train,val,test],ignore_index=True)\n",
    "    print (train.shape, val.shape, test.shape, data.shape)\n",
    "else:\n",
    "    data = pd.concat([train,test],ignore_index=True)\n",
    "    print (train.shape, test.shape, data.shape)\n",
    "tac()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We must now remove the target column=Demanda_uni_equil from the val dataset and keep it away\n",
    "data['ix'] = data.index # assign and id column to later at the end map the val_targets back to the dataframe\n",
    "if (use_validation):\n",
    "    val_target = data[['ix','Demanda_uni_equil']].loc[data['source']=='val'] #save the val targets on a different dataframe\n",
    "    data.loc[data['source']=='val','Demanda_uni_equil'] =  np.nan # replaces val targets with nan for feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can see that data has same #columns but rows equivalent to both test and train. Lets start by checking which columns contain missing values. (takes aprox 30 mins to run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data.apply(lambda x: sum(x.isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be any missing values (other than the NaN we set on the test and train sets).\n",
    "\n",
    "Lets look at some basic statistics for numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some observations:\n",
    "\n",
    "   Looking at Demanda_uni_equil (our target), or the amount of product sold per week, we find interesting things:\n",
    "   \n",
    "   **1)** The average is 7.22, so in average there is 7 units per week per store sold.\n",
    "   \n",
    "   **2)** Looking at the max of 5000, it looks very far fro the mean (3 orders of magnitude), so we must check for an outlier here or a store that is crazy different from the rest.\n",
    "   \n",
    "   **3)** Same behaviour we find on Dev_uni_proxima, Venta_hoy and Venta_uni_hoy\n",
    "   \n",
    "Looking at the nice data analysis made in R by Faviens, here: https://www.kaggle.com/fabienvs/grupo-bimbo-inventory-demand/notebook-8a62eda039a3b0b944cf/notebook we corroborate the outlier(s):\n",
    "There is a massive client: Puebla Remision\n",
    "   \n",
    "![Image of size of Customers]( https://www.kaggle.io/svf/267812/783a24d1dd546819a44914f996b249e8/__results___files/figure-html/unnamed-chunk-16-1.png)\n",
    "   \n",
    "\n",
    "Moving to nominal (categorical) variable, lets have a look at the number of unique values in each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, in train and test sets, we have 552 Agencies(depots), 890k clients (we might have some repeated clients due to typos when enterind data), 1833 products (we might have some repeated products here based on typos) and 3620 routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see how many records we have per week\n",
    "for i in range(3,12):\n",
    "    print(\"Semana\"+repr(i)+\" =\\t\" + repr(data[data[\"Semana\"]==i].Semana.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in the Kaggle competition - Week 10 and 11 is sampled down aprox 70%. According to Kaggle, this was done so the scoring of candidates didn't take extremely long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\\. Data Cleaning\n",
    "\n",
    "This step involves imputing missing values and treating outliers. As we saw before, there are no missing values. Regarding outliers, there seem to be an obvious one, but we are going to see later on if its necessary to treat it differently.\n",
    "\n",
    "My initial reaction would be to see if anything with the word REMISION is on the test set. if not, then delete it. See this discussion: https://www.kaggle.com/c/grupo-bimbo-inventory-demand/forums/t/22037/puebla-remission/126053"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's find out who are the clients with the word REMISION on it\n",
    "client_name = pd.read_csv('./input-data/cliente_tabla.csv')\n",
    "cliente_id_name_train = pd.merge(train,client_name, on='Cliente_ID')\n",
    "cliente_id_name_test = pd.merge(test,client_name, on='Cliente_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cliente_id_name_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cliente_id_name_train[cliente_id_name_train.NombreCliente.str.contains('REMISION')].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the word \"REMISION\" shows up 140k times on the train set. Let's see the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cliente_id_name_test[cliente_id_name_test.NombreCliente.str.contains('REMISION')].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12k rows shows up the word REMISION on the test set. This implies that it has to be predicted as well. We cannot eliminate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\\. Feature Engineering\n",
    "\n",
    "We explored some nuances in the data in the data exploration section. Lets move on to resolving them and making our data ready for analysis. We will also create some new variables using the existing ones in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First thing we need to do is to transform our target ( Demanda_uni_equil) to log(1 + demand) - this makes sense since we're \n",
    "#trying to minimize rmsle vs the mean which minimizes rmse. At the end of the modeling (for submission) we need to reverse it \n",
    "#by applying expm1(x)\n",
    "data['log_target'] = np.log1p(data[\"Demanda_uni_equil\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's also create all the grouping dataframes we are going to need \n",
    "tic()\n",
    "\n",
    "global_mean = data['log_target'].mean()\n",
    "prod_mean = data.groupby('Producto_ID').agg({'log_target': 'mean' })\n",
    "client_mean = data.groupby('Cliente_ID').agg({'log_target': 'mean' })\n",
    "prod_client_mean = data.groupby(['Producto_ID', 'Cliente_ID']).agg({'log_target': 'mean' })\n",
    "semana_client_prod_mean = data.groupby(['Semana','Cliente_ID','Producto_ID']).agg({'log_target': 'mean'})\n",
    "ruta_cliente_prod_mean = data.groupby(['Ruta_SAK','Cliente_ID','Producto_ID']).agg({'log_target': 'mean'})\n",
    "\n",
    "tac()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature 1:  Mean of pairs cliente-producto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First and foremost, we want to create the average (mean) demand per each pair of cliente-producto, if we don't have the demand of the pair,  we calculate the average demand of the product, and if we don't have the demand of the product(new introduced product that week), then we calculate the average demand of the client, and if we don't have any (new client and  on that week), we default to the global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic()\n",
    "prod_mean_dict = prod_mean.to_dict()\n",
    "client_mean_dict = client_mean.to_dict()\n",
    "prod_client_mean_dict = prod_client_mean.to_dict()\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_pairs_mean_feature(key):\n",
    "    key = tuple(key)\n",
    "    product = key[0]\n",
    "    client = key[1]\n",
    "    \n",
    "    val = prod_client_mean_dict['log_target'][(product,client)]\n",
    "    if np.isnan(val):\n",
    "        val = prod_mean_dict['log_target'][(product)]\n",
    "        if np.isnan(val):\n",
    "            val = client_mean_dict['log_target'][(client)]\n",
    "            if np.isnan(val):\n",
    "                val = global_mean\n",
    "            \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not (retrieve_saved_work):\n",
    "    tic()\n",
    "    data['pairs_mean'] = data[['Producto_ID', 'Cliente_ID']].apply(lambda x:gen_pairs_mean_feature(x), axis=1)\n",
    "    tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Saving the work\n",
    "if not (retrieve_saved_work):\n",
    "    tic()\n",
    "    output = open('./input-data/Data_pairs_mean.pkl', 'wb')\n",
    "    pickle.dump(data, output)\n",
    "    tac()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 2: Lags - Demand per client-product pair for prior weeks\n",
    "Based on this blog: http://blog.nycdatascience.com/student-works/predicting-demand-historical-sales-data-grupo-bimbo-kaggle-competition/\n",
    "\n",
    "As this script said: https://www.kaggle.com/bpavlyshenko/grupo-bimbo-inventory-demand/bimbo-xgboost-r-script-lb-0-457/code\n",
    "It is important to know what were the previous weeks sales. If the previous week, too many products were supplied and they were not sold, the next week this product amount, supplied to the same store, will be decreased. So it is very important to included lag values of target variable as a feature to predict the next sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Retrieve the saved work\n",
    "if (retrieve_saved_work):\n",
    "    tic()\n",
    "    pkl_file = open('./input-data/Data_pairs_mean.pkl', 'rb')\n",
    "    data = pickle.load(pkl_file)\n",
    "    tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = semana_client_prod_mean.reset_index() # we convert the index to columns for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see how many records we have per week on the semana_cliente_Producto groups vs the raw dataset\n",
    "for i in range(3,12):\n",
    "    print(\"Semana\"+repr(i)+\" =\\t\" + repr(data[data[\"Semana\"]==i].Semana.count())+ \" (raw)\\t\" +\n",
    "            repr(df[df[\"Semana\"]==i].Semana.count()) + \" (group)\\t\" +  \n",
    "            repr(data[data[\"Semana\"]==i].Semana.count() - df[df[\"Semana\"]==i].Semana.count()) + \" (diff)\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, there are repeated combinations of client-product on each week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Before we start adding lags and removing rows, let's see the size of our dataset\n",
    "size_data = data.memory_usage().sum()\n",
    "print(size_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#here we add the number of lags we want\n",
    "tic()\n",
    "\n",
    "for i in range(1,lag+1):\n",
    "    df['Semana'] += 1\n",
    "    df.rename(columns={df.columns[3]: 'Log_Target_mean_lag%d' %(i)}, inplace=True)\n",
    "    data = pd.merge(data,df, how = 'left', on = ['Semana','Cliente_ID','Producto_ID']) #here we add the lag to the dataset\n",
    "    data['Log_Target_mean_lag%d' %(i)].fillna(0, inplace=True) # we replace the client-product log mean NaN/Not found on the week before with ZERO\n",
    "    if(trimmed): data = data[data.Semana != i+2] # here we delete the week rows we dont have lags for\n",
    "   \n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's see how many NaN or Nulls we have\n",
    "#data.apply(lambda x: sum(x.isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above looks correct! the only NaN shown are the variables that are not avaibable on the val or test set, everyting else looks good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see how many records we have per week and make sure we didn't delete any data from our important weeks\n",
    "for i in range(3,12):\n",
    "    print(\"Semana\"+repr(i)+\" =\\t\" + repr(data[data[\"Semana\"]==i].Semana.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above looks correct! we deleted the week rows we don't have lags for, and we kept the original amount of rows for the remaining weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now let's see how much was the data set size reduced/increased\n",
    "new_size_data = data.memory_usage().sum()\n",
    "print(\"Dataset size changed in \" + repr((new_size_data - size_data )*100/new_size_data) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! the data set was greatly reduced (this means faster modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature 4:  Calculates de sum of prior weeks Log mean Demands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We want to sum the lags up until week 9, this means that we need to sum lag2 and up.\n",
    "data['Lags_sum'] = 0\n",
    "for i in range(1,lag+1):\n",
    "    data['Lags_sum'] += data['Log_Target_mean_lag%d' %(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 5: Create a broad category of Brand of item (brand hypothesis)\n",
    "Let's preprocess products a little bit. I borrowed some of the preprocessing from here: https://www.kaggle.com/vykhand/grupo-bimbo-inventory-demand/exploring-products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products  =  pd.read_csv(\"input-data/producto_tabla.csv\")\n",
    "products  =  pd.read_csv(\"input-data/producto_tabla.csv\")\n",
    "#products['short_name'] = products.NombreProducto.str.extract('^(\\D*)', expand=False)#python 2.7\n",
    "products['short_name'] = products.NombreProducto.str.extract('^(\\D*)')#python 3.0\n",
    "#products['brand'] = products.NombreProducto.str.extract('^.+\\s(\\D+) \\d+$', expand=False)\n",
    "products['brand'] = products.NombreProducto.str.extract('^.+\\s(\\D+) \\d+$')\n",
    "#w = products.NombreProducto.str.extract('(\\d+)(Kg|g)', expand=True)\n",
    "w = products.NombreProducto.str.extract('(\\d+)(Kg|g)')\n",
    "products['weight'] = w[0].astype('float')*w[1].map({'Kg':1000, 'g':1})\n",
    "#products['pieces'] =  products.NombreProducto.str.extract('(\\d+)p ', expand=False).astype('float')\n",
    "products['pieces'] =  products.NombreProducto.str.extract('(\\d+)p ').astype('float')\n",
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products.brand.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products.brand.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products_id_brand  = products[['Producto_ID', 'brand']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.merge(data, products_id_brand, on='Producto_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 6: Create clusters of Products (utility hypothesis) - ramdonly pick 30 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read files:\n",
    "product_clusters = pd.read_csv('input-data/producto_clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "product_clusters.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (product_clusters[\"cluster\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products_id_clusters = product_clusters[['Producto_ID', 'cluster']].copy()\n",
    "products_id_clusters.rename(columns={'cluster': 'prodtype_cluster'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products_id_clusters.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.merge(data, products_id_clusters, on='Producto_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 7: Create a category of Size of store based on Number of Agencies and Routes and Sales Channels that serve the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Determine pivot table\n",
    "Rutas_per_store = data.pivot_table(values=[\"Ruta_SAK\"], index=[\"Cliente_ID\"], aggfunc=pd.Series.nunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Rutas_per_store.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Agencies_per_store = data.pivot_table(values=[\"Agencia_ID\"], index=[\"Cliente_ID\"], aggfunc=pd.Series.nunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Agencies_per_store.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Canals_per_store = data.pivot_table(values=[\"Canal_ID\"], index=[\"Cliente_ID\"], aggfunc=pd.Series.nunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Canals_per_store.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't look that we can Bin on Canal_ID or Agencia_ID since they are not at least semi evenly ditributed, but it does look like Ruta_SAK is a good option based on the percentiles distribution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Rutas_per_store.rename(columns={'Ruta_SAK': 'Qty_Ruta_SAK'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mergin Routa_Sak's per client to data df\n",
    "data = pd.merge(data,Rutas_per_store,right_index=True, left_on='Cliente_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Binning:\n",
    "def binning(col, cut_points, labels=None):\n",
    "  #Define min and max values:\n",
    "  minval = col.min()\n",
    "  maxval = col.max()\n",
    "\n",
    "  #create list by adding min and max to cut_points\n",
    "  break_points = [minval] + cut_points + [maxval]\n",
    "\n",
    "  #if no labels provided, use default labels 0 ... (n-1)\n",
    "  if not labels:\n",
    "    labels = range(len(cut_points)+1)\n",
    "\n",
    "  #Binning using cut function of pandas\n",
    "  colBin = pd.cut(col,bins=break_points,labels=labels,include_lowest=True)\n",
    "  return colBin\n",
    "\n",
    "#Binning Qty_Ruta_SAK:\n",
    "cut_points = [2,4,10]\n",
    "labels = [\"low\",\"medium\",\"high\",\"very high\"]\n",
    "data[\"Qty_Ruta_SAK_Bin\"] = binning(data[\"Qty_Ruta_SAK\"], cut_points, labels)\n",
    "print (pd.value_counts(data[\"Qty_Ruta_SAK_Bin\"], sort=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We don't need Qty_Ruta_Sak anymore\n",
    "data.drop(['Qty_Ruta_SAK'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 8: Create a category of location based on zip code (embedded on town table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import os\n",
    "import time\n",
    "towns = pd.read_csv(\"input-data/town_state.csv\")\n",
    "L = lambda x: list(map(int, re.findall('\\d+', x)))[0]\n",
    "towns['ZipCode'] = towns.Town.apply(L) \n",
    "towns['ZipCode'] = np.uint16(towns['ZipCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zipcodes_df = towns[['Agencia_ID', 'ZipCode']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zipcodes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.merge(data, zipcodes_df, on='Agencia_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 9: Week of the month counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to have an indicator of what week of the month the current data belongs. This is to see if there is a monthly pattern that the algorithm can pick up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['week_ct'] = data['Semana'].apply(lambda x: x%4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 10: Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.1 Client type clusters\n",
    "Thanks to AbderRahman Sobh - https://www.kaggle.com/abbysobh/grupo-bimbo-inventory-demand/classifying-client-type-using-client-names/comments\n",
    "for doing the great code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client_types = pd.read_csv('./input-data/client_types.csv',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client_types.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.merge(client_types.drop_duplicates(subset=['Cliente_ID']), how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2  Clusters based on demand\n",
    "We are going to load a previous calculate tables of Agencia,Ruta, Producto and Cliente Clusters based on demand mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "import imp\n",
    "from h2o.estimators.kmeans import H2OKMeansEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start a local instance of the H2O engine.\n",
    "h2o.init();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import math as math\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try to find the right amount of clusters\n",
    "\n",
    "There are three diagnostics that will be used to help with determining the number of clusters: total within cluster sum of squares, AIC, and BIC.\n",
    "\n",
    "Total within cluster sum of squares measures sums the distance from each point in a cluster to that point's assigned cluster center. This is the minimization criteria of kmeans. The standard guideline for picking the number of clusters is to look for a 'knee' in the plot, showing where the total within sum of squares stops decreasing rapidly. Total within cluster sum of squares can be difficult to intepret, with the criteria being to look for an arbitrary knee in the plot.\n",
    "\n",
    "With this challenge from total within cluster sum of squares, we will also use two merit statistics for determining the number of clusters. AIC and BIC are both measures of the relative quality of a statistical model. AIC and BIC introduce penality terms for the number of parameters in the model to counter the problem of overfitting; BIC has a larger penality term than AIC. With these merit statistics one is to look for the number of clusters that minimize the statistic.\n",
    "\n",
    "Here we build a method for extracting the inputs for each diagnostics and calculating the AIC and BIC values on a model. Each model is then inspected by the method and the results plotted for quick analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diagnostics_from_clusteringmodel(model):\n",
    "    total_within_sumofsquares = model.tot_withinss()\n",
    "    number_of_clusters = len(model.centers())\n",
    "    number_of_dimensions = len(model.centers()[0])\n",
    "    number_of_rows = sum(model.size())\n",
    "    \n",
    "    aic = total_within_sumofsquares + 2 * number_of_dimensions * number_of_clusters\n",
    "    bic = total_within_sumofsquares + math.log(number_of_rows) * number_of_dimensions * number_of_clusters\n",
    "    \n",
    "    return {'Clusters':number_of_clusters,\n",
    "            'Total Within SS':total_within_sumofsquares, \n",
    "            'AIC':aic, \n",
    "            'BIC':bic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_and_plot(feat_name, k_range):\n",
    "\n",
    "    global data\n",
    "    \n",
    "    # run clustering by demand using all data set and our pairs_mean feature\n",
    "    feat = data.loc[:,feat_name]\n",
    "    tar = data.loc[:,'pairs_mean']\n",
    "    feat_tar = pd.concat([feat, tar], axis=1)\n",
    "        \n",
    "    # group by feature\n",
    "    grouped_ft = feat_tar.groupby(feat_name)\n",
    "    grouped_ft = pd.merge(grouped_ft.median().reset_index(), grouped_ft.std().reset_index(), how='left', on=[feat_name])\n",
    "    \n",
    "    demand_info_unique = grouped_ft.iloc[:,1:]\n",
    "    demand_info_unique[pd.isnull(demand_info_unique)] = 0\n",
    "        \n",
    "    # we use the kmeans clustering algorithm\n",
    "    h2odf = h2o.H2OFrame(demand_info_unique)\n",
    "    results = [H2OKMeansEstimator(k=clusters, init=\"Random\", seed=2, standardize=True) for clusters in k_range]\n",
    "    for estimator in results:\n",
    "        estimator.train(x = h2odf.col_names, training_frame = h2odf)\n",
    "\n",
    "    diagnostics = pd.DataFrame( [diagnostics_from_clusteringmodel(model) for model in results])\n",
    "    diagnostics.set_index('Clusters', inplace=True)\n",
    "    diagnostics.plot(kind='line');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agencia_k_range = range(10, 50, 5)\n",
    "tic()\n",
    "cluster_and_plot('Agencia_ID', agencia_k_range)\n",
    "tac()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It Looks Like Clusters = 20 is a good choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ruta_k_range = range(60, 300, 30)\n",
    "tic()\n",
    "cluster_and_plot('Ruta_SAK', ruta_k_range)\n",
    "tac()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It Looks Like Clusters = 80 is a good choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "producto_k_range = range(30, 150, 15)\n",
    "tic()\n",
    "cluster_and_plot('Producto_ID', producto_k_range)\n",
    "tac()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It Looks Like Clusters = 45 is a good choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic()\n",
    "cliente_k_range = [10,100,300,500]\n",
    "#cluster_and_plot('Cliente_ID', cliente_k_range)\n",
    "tac()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like 300 or 400 would work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we the right amount of centroids, we process the clustering and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- HYPERPARAMETERS FOR K-MEANS ---\n",
    "\n",
    "# number of clusters to group depot/route/produc (if 0 will not be added as feature)\n",
    "num_clusters_agencia = 20\n",
    "num_clusters_ruta = 80\n",
    "num_clusters_producto = 45\n",
    "num_clusters_cliente = 300 # Amount of centroids for k-means client clustering by demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_and_save(feat_name, num_clusters):\n",
    "    '''\n",
    "    Input: \n",
    "        - idx: the index of the feature we want to cluster, one-hot-encode, and add to our features\n",
    "        - num_clusters: the number of clusters we want to use to group the feature values\n",
    "    '''   \n",
    "    global data\n",
    "\n",
    "    # run clustering by demand using info from week 3-9\n",
    "    feat = data.loc[:, feat_name]\n",
    "    tar = data.loc[:, 'pairs_mean']\n",
    "    feat_tar = pd.concat([feat, tar], axis=1)\n",
    "        \n",
    "    # group by feature\n",
    "    grouped_ft = feat_tar.groupby(feat_name)\n",
    "    grouped_ft = pd.merge(grouped_ft.median().reset_index(), grouped_ft.std().reset_index(), how='left', on=[feat_name])\n",
    "    \n",
    "    demand_info_unique = grouped_ft.iloc[:,1:]\n",
    "    demand_info_unique[pd.isnull(demand_info_unique)] = 0\n",
    "    \n",
    "    # we use the kmeans clustering algorithm\n",
    "    h2odf = h2o.H2OFrame(demand_info_unique)\n",
    "    kmeans = H2OKMeansEstimator(k=num_clusters, init=\"Random\", seed=2, standardize=True)\n",
    "\n",
    "    kmeans.train(x = h2odf.col_names, training_frame = h2odf)\n",
    "    clusters_h2odf = kmeans.predict(h2odf)\n",
    "    clustersdf = clusters_h2odf.as_data_frame(True)\n",
    "    clusters = clustersdf.predict.get_values()\n",
    "    \n",
    "    # plot demand/cluster\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for c in range(num_clusters):\n",
    "        d_median = demand_info_unique.iloc[clusters==c,0]\n",
    "        d_std = demand_info_unique.iloc[clusters==c,1]\n",
    "        plt.plot(d_median,d_std,'.')\n",
    "        plt.xlabel('median')\n",
    "        plt.ylabel('std')\n",
    "    plt.savefig(\"./input-data/h2o-clustByDem_{}_{}\".format(feat_name, num_clusters))\n",
    "    \n",
    "    # create new dataframe to save the mapping from feature ID to cluster ID\n",
    "    feat_clust_map = pd.DataFrame(data = grouped_ft.iloc[:,0], columns=[feat_name])\n",
    "    feat_clust_map.insert(1, feat_name+'_clust_ID', clusters)\n",
    "\n",
    "    # save the new feature in files for others to use \n",
    "    feat_clust_map.to_csv(\"./input-data/h2o-clustByDem_{}.csv\".format(feat_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic()\n",
    "print(\"Agencia Clusters\")\n",
    "cluster_and_save('Agencia_ID', num_clusters_agencia)\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic()\n",
    "print(\"Ruta Clusters\")\n",
    "cluster_and_save('Ruta_SAK', num_clusters_ruta)\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic()\n",
    "print(\"Producto Clusters\")\n",
    "cluster_and_save('Producto_ID', num_clusters_producto)\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic()\n",
    "print(\"Cliente Clusters\")\n",
    "cluster_and_save('Cliente_ID', num_clusters_cliente)\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read files:\n",
    "agencia_by_dem_clust = pd.read_csv('input-data/h2o-clustByDem_Agencia_ID.csv')\n",
    "ruta_by_dem_clust = pd.read_csv('input-data/h2o-clustByDem_Ruta_SAK.csv')\n",
    "prod_by_dem_clust = pd.read_csv('input-data/h2o-clustByDem_Producto_ID.csv')\n",
    "cliente_by_dem_clust = pd.read_csv('input-data/h2o-clustByDem_Cliente_ID.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.merge(data, prod_by_dem_clust, on='Producto_ID')\n",
    "data = pd.merge(data, ruta_by_dem_clust, on='Ruta_SAK')\n",
    "data = pd.merge(data, agencia_by_dem_clust, on='Agencia_ID')\n",
    "data = pd.merge(data, cliente_by_dem_clust, on='Cliente_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Encode nominal features, one-hot Categorical features and Scale numerical features\n",
    "Since scikit-learn accepts only numerical variables, so i have to convert all categories of nominal variables into numeric types.\n",
    "\n",
    "Lets start with coding all low cardinality object/nominal categorical variables (brand, Qty_Ruta_SAK_Bin, NombreCliente)  as numeric using ‘LabelEncoder’ from sklearn’s preprocessing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Here we convert to numbers the nominal variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "var_mod = ['brand', 'Qty_Ruta_SAK_Bin', 'NombreCliente']\n",
    "for i in var_mod:\n",
    "    data[i] = le.fit_transform(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One-Hot-Coding refers to creating dummy variables, one for each category of a categorical variable. For example, the 'cluster' variable has 30 categories. One hot coding will remove this variable and generate 30 new variables. Each will have binary numbers – 0 (if the category is not present) and 1(if category is present).\n",
    "Categorical variables are intentionally (for censorship) or implicitly encoded as numerical variables in order to be used as features in any given model.\n",
    "\n",
    "e.g. [house, car, tooth, car] becomes [0,1,2,1].\n",
    "\n",
    "This imparts an ordinal property to the variable, i.e. house < car < tooth.\n",
    "\n",
    "As this is ordinal characteristic is usually not desired, one hot encoding is necessary for the proper representation of the distinct elements of the variable.\n",
    "\n",
    "-- This can be done using ‘get_dummies’ function of Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One Hot Coding of the categorical variables\n",
    "onehot_categoricals = False # We don't need to do this now. It is better to do it in the modeling part, and use RAM not disk space\n",
    "if (onehot_categoricals):\n",
    "    tic()\n",
    "    data = pd.get_dummies(data, columns=['week_ct','prodtype_cluster','Qty_Ruta_SAK_Bin', 'brand', 'NombreCliente','ZipCode'])\n",
    "    tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Scaling of the numerical values\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "s_cols = ['pairs_mean', 'Lags_sum'] # s_cols is an array of all numerical features\n",
    "for i in range(1,lag):\n",
    "    s_cols.insert(0,'Log_Target_mean_lag{}'.format(i))\n",
    "\n",
    "data[s_cols] = min_max_scaler.fit_transform(data[s_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\\. Exporting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Divide into test and train:\n",
    "\n",
    "tic()\n",
    "train = data.loc[data['source']==\"train\"]\n",
    "test = data.loc[data['source']==\"test\"]\n",
    "if (use_validation):\n",
    "    val = data.loc[data['source']==\"val\"]\n",
    "    val.drop('Demanda_uni_equil',axis=1,inplace=True) # here all Demanda_uni_equil = nan\n",
    "    # we merge back the val_targets to the val dataset\n",
    "    val = pd.merge(val,val_target, on='ix') # we add the true Demanda_uni_equil back\n",
    "    val['log_target'] = np.log1p(val[\"Demanda_uni_equil\"]) # adds the log_target col\n",
    "\n",
    "#Drop unnecessary columns: note - we are dropping Demanda_uni_equil since we replaced it by log_target\n",
    "train.drop(['id','Demanda_uni_equil','source','ix'],axis=1,inplace=True)\n",
    "test.drop(['Demanda_uni_equil','source','ix'],axis=1,inplace=True)\n",
    "if (use_validation):\n",
    "    val.drop(['id','Demanda_uni_equil','source','ix'],axis=1,inplace=True)\n",
    "\n",
    "#Export files as modified versions:\n",
    "sufix=\"\"\n",
    "if (use_validation): \n",
    "    sufix += \"_holdout\"\n",
    "    sufix += repr(val_week_threshold)\n",
    "if (trimmed): sufix += \"_trimmed\"\n",
    "\n",
    "print(\"Writing: train_modified\"+sufix+\".csv  to disk ...\")\n",
    "train.to_csv(\"./input-data/train_modified\"+sufix+\".csv\", index=False, quoting=csv.QUOTE_NONE)\n",
    "print(\"Writing: test_modified\"+sufix+\".csv  to disk ...\")\n",
    "test.to_csv(\"./input-data/test_modified\"+sufix+\".csv\", index=False, quoting=csv.QUOTE_NONE)\n",
    "if (use_validation):\n",
    "    print(\"Writing: val_modified\"+sufix+\".csv  to disk ...\")\n",
    "    val.to_csv(\"./input-data/val_modified\"+sufix+\".csv\", index=False, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
