{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model Building in H2O\n",
    "\n",
    "I will go through 2 H2O  models including  GBM, and DL (Deep Learning NN).\n",
    "\n",
    "I'll use H2OFlow for the hyperparameters searching (it's just easier than writing code) and post here the best parameters found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "import math\n",
    "\n",
    "_start_time = time.time()\n",
    "\n",
    "def tic():\n",
    "    global _start_time \n",
    "    _start_time = time.time()\n",
    "\n",
    "def tac():\n",
    "    t_sec = round(time.time() - _start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60) \n",
    "    print('Time passed: {}hour:{}min:{}sec'.format(t_hour,t_min,t_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "import time\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to H2O server at http://localhost:54321....... failed.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_91\"; OpenJDK Runtime Environment (build 1.8.0_91-b14); OpenJDK 64-Bit Server VM (build 25.91-b14, mixed mode)\n",
      "  Starting server from /anaconda/envs/py35/h2o_jar/h2o.jar\n",
      "  Ice root: /tmp/tmp62r9u06d\n",
      "  JVM stdout: /tmp/tmp62r9u06d/h2o_dsvm_started_from_python.out\n",
      "  JVM stderr: /tmp/tmp62r9u06d/h2o_dsvm_started_from_python.err\n",
      "Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful!\n"
     ]
    }
   ],
   "source": [
    "# Connect to a cluster\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's define variables that will define the behaviour of the whole script\n",
    "s3_path = 'http://bbts-kaggle.s3.amazonaws.com/bimbo/Pablo/'\n",
    "use_validation=True # splits train data into train + val sets\n",
    "val_week_threshold = 8 # (possible values 8 or 9)  - weeks 3,4,5,6,7 are train, and week 8.9 are val\n",
    "trimmed = True # removes weeks which doesn't have all the lags. If False, fills empty lags with 0\n",
    "lag = 5  # shifted mean_demand up to \"lag\" weeks\n",
    "if (val_week_threshold == 8): lag = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading File: http://bbts-kaggle.s3.amazonaws.com/bimbo/Pablo/jorge_train.csv ...\n"
     ]
    }
   ],
   "source": [
    "#now we load our modified train and test set\n",
    "tic()\n",
    "sufix=\"\"\n",
    "sufix=\"\"\n",
    "if (use_validation): \n",
    "    sufix += \"_holdout\"\n",
    "    sufix += repr(val_week_threshold)\n",
    "if (trimmed): sufix += \"_trimmed\"\n",
    "\n",
    "train_csv = s3_path +\"train_modified\"+sufix+\".csv\"\n",
    "val_csv = s3_path +\"val_modified\"+sufix+\".csv\"\n",
    "test_csv = s3_path +\"test_modified\"+sufix+\".csv\"\n",
    "\n",
    "tic()\n",
    "print ('Downloading File: {} ...'.format(train_csv))\n",
    "train = h2o.import_file(train_csv)\n",
    "\n",
    "if (use_validation):\n",
    "    print ('Downloading File: {} ...'.format(val_csv))\n",
    "    val = h2o.import_file(val_csv)\n",
    "    \n",
    "print ('Downloading File: {} ...'.format(test_csv))\n",
    "test = h2o.import_file(test_csv)\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define target and ID columns:\n",
    "target = 'log_target'\n",
    "IDcol = 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# H2O python API recently (Jun 2016) added RSME as a model performance metric. So we are going to use it directly\n",
    "# into our target = log_target , to get the RSMLE\n",
    "\n",
    "def modelfit(alg, dtrain, dval, dtest, predictors, target, IDcol, filename):   \n",
    "    #Fit the algorithm on the data\n",
    "    alg.train(x=predictors, y=target, training_frame=dtrain, validation_frame=dval)\n",
    "    \n",
    "    #Performance on Training and Val sets:\n",
    "    print (\"\\nModel Report\")\n",
    "    print ('RMSLE TRAIN: ', alg.model_performance(train).rmse())\n",
    "    print ('RMSLE VAL: ', alg.model_performance(val).rmse())\n",
    " \n",
    "    #Predict on testing data: we need to revert it back to \"Demanda_uni_equil\" by applying expm1 \n",
    "    dtest[target] = alg.predict(dtest[predictors]).expm1()\n",
    "    \n",
    "    print ('NUM ROWS PREDICTED: ', dtest.shape[0] )\n",
    "    print ('MIN TARGET PREDICTED: ', dtest[target].min())\n",
    "    print ('MEAN TARGET PREDICTED: ', dtest[target].mean())\n",
    "    print ('MAX TARGET PREDICTED: ', dtest[target].max())\n",
    "    \n",
    "    \n",
    "    #Export submission file:\n",
    "    submission = dtest[[IDcol,target]].as_data_frame(use_pandas=True)\n",
    "    submission[target] = np.maximum(submission[target], 0) # we make all negative numbers = 0 since there cannot be a negative demand\n",
    "    submission[IDcol] = submission[IDcol].astype(int)\n",
    "    submission.rename(columns={target: 'Demanda_uni_equil'}, inplace=True)\n",
    "    submission.to_csv(\"./Submissions/\"+filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define now the target and the Id cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#H@O would automatically hot-encode the categorical values (Genius!), but we must specify what columns are categoricals\n",
    "train['brand'] = train['brand'].asfactor()\n",
    "train['prodtype_cluster'] = train['prodtype_cluster'].asfactor()\n",
    "train['ZipCode'] = train['ZipCode'].asfactor()\n",
    "train['week_ct'] = train['week_ct'].asfactor()\n",
    "train['NombreCliente'] = train['NombreCliente'].asfactor()\n",
    "train['Producto_ID_clust_ID'] = train['Producto_ID_clust_ID'].asfactor()\n",
    "train['Ruta_SAK_clust_ID'] = train['Ruta_SAK_clust_ID'].asfactor()\n",
    "train['Agencia_ID_clust_ID'] = train['Agencia_ID_clust_ID'].asfactor()\n",
    "train['Cliente_ID_clust_ID'] = train['Cliente_ID_clust_ID'].asfactor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In case there is no validation, we make val = train\n",
    "if not (use_validation):\n",
    "    val = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alg8 - GBM\n",
    "\n",
    "Lets make our first GBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = ['Canal_ID', 'Log_Target_mean_lag1', 'Log_Target_mean_lag2', 'Log_Target_mean_lag3', 'Log_Target_mean_lag4', \n",
    "              'Agencia_ID','Ruta_SAK','Cliente_ID','Producto_ID',\n",
    "              'Lags_sum', 'brand', 'prodtype_cluster', 'Qty_Ruta_SAK_Bin', 'ZipCode', 'Producto_ID_clust_ID']\n",
    "\n",
    "\n",
    "model = H2OGradientBoostingEstimator(ntrees=300,max_depth=10,learn_rate=0.1, min_rows=10, nbins=40, sample_rate=0.7,\n",
    "                                    col_sample_rate=0.7, stopping_rounds=10, stopping_metric=\"MSE\", stopping_tolerance=0.01)\n",
    "tic()\n",
    "modelfit(model, train, val, test, predictors, target, IDcol, 'alg8_{}.csv'.format(sufix))\n",
    "tac()\n",
    "\n",
    "model.varimp(use_pandas=True)\n",
    "#Plot Coeficients importance\n",
    "#coef = pd.Series(model.feature_importances_, predictors).sort_values(ascending=False)\n",
    "#coef.plot(kind='bar', title='Feature Importances')\n",
    "\n",
    "#Plot Histogram of target and prediction distributions\n",
    "model_history_df = model.scoring_history()\n",
    "model_history_df\n",
    "plt.plot(model_history_df['training_rmse'], label=\"training_rmse\")\n",
    "plt.plot(model_history_df['validation_rmse'], label=\"validation_rmse\")\n",
    "plt.title(\"Deep Learner .. )\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --> LB: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alg9 - Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with a Deep Learning Network. To improve generalization we added dropout and L1 and L2 penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = ['Canal_ID', 'Log_Target_mean_lag1', 'Log_Target_mean_lag2', 'Log_Target_mean_lag3', 'Log_Target_mean_lag4', \n",
    "              'Agencia_ID','Ruta_SAK','Cliente_ID','Producto_ID',\n",
    "              'Lags_sum', 'brand', 'prodtype_cluster', 'Qty_Ruta_SAK_Bin', 'ZipCode', 'Producto_ID_clust_ID']\n",
    "\n",
    "model = H2ODeepLearningEstimator(activation=\"Rectifier\", hidden=[100,100,100], epochs=35,\n",
    "                                standardize=False, score_interval=10, stopping_rounds=10, stopping_metric=\"MSE\",\n",
    "                                stopping_tolerance=0.01, use_all_factor_levels=False)\n",
    "    \n",
    "tic()\n",
    "modelfit(model, train, val, test, predictors, target, IDcol, 'alg9_{}.csv'.format(sufix))\n",
    "tac()\n",
    "\n",
    "model.varimp(use_pandas=True)\n",
    "#Plot Coeficients importance\n",
    "#coef = pd.Series(model.feature_importances_, predictors).sort_values(ascending=False)\n",
    "#coef.plot(kind='bar', title='Feature Importances')\n",
    "\n",
    "#Plot Histogram of target and prediction distributions\n",
    "model_history_df = model.scoring_history()\n",
    "model_history_df\n",
    "plt.plot(model_history_df['training_rmse'], label=\"training_rmse\")\n",
    "plt.plot(model_history_df['validation_rmse'], label=\"validation_rmse\")\n",
    "plt.title(\"Deep Learner .. )\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
