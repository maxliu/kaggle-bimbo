{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Building in XGBoost\n",
    "\n",
    "This is a great article for tunning XGboost: http://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "windows=False\n",
    "if (windows):\n",
    "    mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-5.3.0-posix-seh-rt_v4-rev0\\\\mingw64\\\\bin'\n",
    "    os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "import math\n",
    "\n",
    "_start_time = time.time()\n",
    "\n",
    "def tic():\n",
    "    global _start_time \n",
    "    _start_time = time.time()\n",
    "\n",
    "def tac():\n",
    "    t_sec = round(time.time() - _start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60) \n",
    "    print('Time passed: {}hour:{}min:{}sec'.format(t_hour,t_min,t_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_validation=True\n",
    "scale_numericals=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sufix: _holdout\n",
      "Time passed: 0hour:0min:52sec\n"
     ]
    }
   ],
   "source": [
    "#now we load our modified train and test set\n",
    "tic()\n",
    "sufix=\"\"\n",
    "if (use_validation): sufix += \"_holdout\"\n",
    "if (scale_numericals): sufix += \"_scaled\"\n",
    "print (\"sufix: \"+sufix)\n",
    "\n",
    "train = pd.read_csv(\"./input-data/train_modified\"+sufix+\".csv\",\n",
    "                    dtype = {'Canal_ID': 'int8',\n",
    "                            'log_target':  'float64',\n",
    "                            'Log_Target_mean_lag1': 'float64',\n",
    "                            'Log_Target_mean_lag2': 'float64',\n",
    "                            'Log_Target_mean_lag3': 'float64',\n",
    "                            'Log_Target_mean_lag4': 'float64',\n",
    "                            'Log_Target_mean_lag5': 'float64',\n",
    "                            'Lags_sum': 'float64',\n",
    "                            'pairs_mean':  'float64',\n",
    "                            'brand': 'int8',\n",
    "                            'prodtype_cluster': 'int32',\n",
    "                            'Qty_Ruta_SAK_Bin': 'int32',\n",
    "                            'ZipCode': 'uint32',\n",
    "                            'week_ct': 'int8',\n",
    "                            'NombreCliente': 'int32',\n",
    "                            'Producto_ID_clust_ID':'int32',\n",
    "                            'Ruta_SAK_clust_ID':'int32',\n",
    "                            'Agencia_ID_clust_ID':'int32',\n",
    "                            'Cliente_ID_clust_ID':'int32'},\n",
    "                   )\n",
    "                  \n",
    "val = pd.read_csv(\"./input-data/val_modified\"+sufix+\".csv\",\n",
    "                    dtype = {'Canal_ID': 'int8',\n",
    "                            'log_target':  'float64',\n",
    "                            'Log_Target_mean_lag1': 'float64',\n",
    "                            'Log_Target_mean_lag2': 'float64',\n",
    "                            'Log_Target_mean_lag3': 'float64',\n",
    "                            'Log_Target_mean_lag4': 'float64',\n",
    "                            'Log_Target_mean_lag5': 'float64',\n",
    "                            'Lags_sum': 'float64',\n",
    "                            'pairs_mean':  'float64',\n",
    "                            'brand': 'int8',\n",
    "                            'prodtype_cluster': 'int32',\n",
    "                            'Qty_Ruta_SAK_Bin': 'int32',\n",
    "                            'ZipCode': 'uint32',\n",
    "                            'week_ct': 'int8',\n",
    "                            'NombreCliente': 'int32',\n",
    "                            'Producto_ID_clust_ID':'int32',\n",
    "                            'Ruta_SAK_clust_ID':'int32',\n",
    "                            'Agencia_ID_clust_ID':'int32',\n",
    "                            'Cliente_ID_clust_ID':'int32'},\n",
    "                   ) \n",
    "    \n",
    "test = pd.read_csv(\"./input-data/test_modified\"+sufix+\".csv\",\n",
    "                    dtype = {'id': 'uint32',\n",
    "                            'Canal_ID': 'int8',\n",
    "                            'Log_Target_mean_lag1': 'float64',\n",
    "                            'Log_Target_mean_lag2': 'float64',\n",
    "                            'Log_Target_mean_lag3': 'float64',\n",
    "                            'Log_Target_mean_lag4': 'float64',\n",
    "                            'Log_Target_mean_lag5': 'float64',\n",
    "                            'Lags_sum': 'float64',\n",
    "                            'pairs_mean':  'float64',\n",
    "                            'brand': 'int8',\n",
    "                            'prodtype_cluster': 'int32',\n",
    "                            'Qty_Ruta_SAK_Bin': 'int32',\n",
    "                            'ZipCode': 'uint32',\n",
    "                            'week_ct': 'int8',\n",
    "                            'NombreCliente': 'int32',\n",
    "                            'Producto_ID_clust_ID':'int32',\n",
    "                            'Ruta_SAK_clust_ID':'int32',\n",
    "                            'Agencia_ID_clust_ID':'int32',\n",
    "                            'Cliente_ID_clust_ID':'int32'},\n",
    "                      )\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define target and ID columns:\n",
    "target = 'log_target'\n",
    "IDcol = 'id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train multiple models per client cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we said on our prior step (Models wiht scikit-learn) that we need to deal with the data set high variance. Let's do this first:\n",
    "\n",
    "Looking at the plot below, created on the clustering-by-demand on the feature engineering notebook, we see that some client clusters behave very differntly from others. So this explain why our model is failing on predicting accurately for all of them.\n",
    "We are going then to create a wrapper function to create as many models as Client Clusters by demand are (Cliente_ID_clust_ID). The scores should be bettter individually, and the concatenation of all 400 models should yield a better overall RSMLE than our baseline 0.47.\n",
    "\n",
    "![Image of Variables vs Hypothesis](./input-data/h2o-clustByDem_Cliente_ID_400.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "def model_fit(alg, ctrain, cval, ctest, predictors, target, IDcol):\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    watchlist = [(cval[predictors], cval[target])]\n",
    "    alg.fit(ctrain[predictors], ctrain[target], eval_set=watchlist, eval_metric='rmse', early_stopping_rounds=20, verbose=False)\n",
    "    \n",
    "    alg.evals_result()\n",
    "\n",
    "    #Predict training set:\n",
    "    ctrain[\"predictions\"] = alg.predict(ctrain[predictors])\n",
    "    ctrain[\"predictions\"] = np.maximum(ctrain[\"predictions\"], 0)\n",
    "\n",
    "    \n",
    "    #Predict validation (holdout) set:\n",
    "    cval[\"predictions\"] = alg.predict(cval[predictors])\n",
    "    cval[\"predictions\"] = np.maximum(cval[\"predictions\"], 0)# we make all negative numbers = 0 since there cannot be a negative demand\n",
    "\n",
    "    \n",
    "    #Predict on testing data: we need to revert it back to target by applying expm1\n",
    "    ctest[target] = alg.predict(ctest[predictors])\n",
    "    ctest[target] = np.maximum(ctest[target], 0) # we make all negative numbers = 0 since there cannot be a negative demand\n",
    "\n",
    "    \n",
    "    return ctrain[[target,\"predictions\"]], cval[[target,\"predictions\"]], ctest[[IDcol,target]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clusters_fit (alg, dtrain, dval, dtest, predictors, target, IDcol):\n",
    "    \n",
    "    train_predictions = pd.DataFrame(index=[target,\"predictions\"])\n",
    "    val_predictions = pd.DataFrame(index=[target,\"predictions\"])\n",
    "    test_predictions = pd.DataFrame(index=[IDcol,target])\n",
    "    \n",
    "    clusters_list = train.Cliente_ID_clust_ID.drop_duplicates().get_values()\n",
    "    \n",
    "    for cluster in clusters_list:\n",
    "        \n",
    "        #we get the cluster train,val, test data\n",
    "\n",
    "        ctrain = dtrain.loc[dtrain[\"Cliente_ID_clust_ID\"] == cluster]\n",
    "        cval   = dval.loc[dval[\"Cliente_ID_clust_ID\"] == cluster]\n",
    "        ctest  = dtest.loc[dtest[\"Cliente_ID_clust_ID\"] == cluster]\n",
    "        \n",
    "        #we train the cluster\n",
    "        ctrain, cval, ctest = model_fit(model, ctrain, cval, ctest, predictors, target, IDcol)\n",
    "        \n",
    "        #concatenate each cluster result\n",
    "        train_predictions = pd.concat([train_predictions,ctrain],ignore_index=True)\n",
    "        val_predictions = pd.concat([val_predictions,cval],ignore_index=True)\n",
    "        test_predictions = pd.concat([test_predictions,ctest],ignore_index=True)\n",
    "        \n",
    "    train_predictions.dropna(axis=0, how='any',inplace=True)\n",
    "    val_predictions.dropna(axis=0, how='any',inplace=True)\n",
    "    test_predictions.dropna(axis=0, how='any',inplace=True)\n",
    "    \n",
    "    return train_predictions[[target,\"predictions\"]], val_predictions[[target,\"predictions\"]], test_predictions[[IDcol,target]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_submit(dtrain, dval, dtest, filename):\n",
    "    \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print ('RMSLE TRAIN: ', np.sqrt(metrics.mean_squared_error(dtrain[target], dtrain[\"predictions\"])))\n",
    "    print ('RMSLE VAL: ', np.sqrt(metrics.mean_squared_error(dval[target], dval[\"predictions\"])))\n",
    "    \n",
    "    #Predict on testing data: we need to revert it back to target by applying expm1\n",
    "    dtest[target] = np.expm1(dtest[target])\n",
    "    dtest[target] = np.maximum(dtest[target], 0) # we make all negative numbers = 0 since there cannot be a negative demand\n",
    "  \n",
    "    \n",
    "    print ('NUM ROWS PREDICTED: ', dtest.shape[0] )\n",
    "    print ('NUM NEGATIVES PREDICTED: ', dtest[target][dtest[target] < 0].count())\n",
    "    print ('MIN TARGET PREDICTED: ', dtest[target].min())\n",
    "    print ('MEAN TARGET PREDICTED: ', dtest[target].mean())\n",
    "    print ('MAX TARGET PREDICTED: ', dtest[target].max())\n",
    "    \n",
    "    #Export submission file:\n",
    "    submission = dtest.copy()\n",
    "    submission[IDcol] = submission[IDcol].astype(int)\n",
    "    submission.rename(columns={target: 'Demanda_uni_equil'}, inplace=True)\n",
    "    submission.to_csv(\"./Submissions/\"+filename, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alg10 - XGB - Train each client cluster separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training each of the client clusters separately and see if we have good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "RMSLE TRAIN:  0.379269412264\n",
      "RMSLE VAL:  0.476648489014\n",
      "NUM ROWS PREDICTED:  6999251\n",
      "NUM NEGATIVES PREDICTED:  0\n",
      "MIN TARGET PREDICTED:  0.0\n",
      "MEAN TARGET PREDICTED:  5.244551531681667\n",
      "MAX TARGET PREDICTED:  2919.23876842\n",
      "Time passed: 0hour:52min:44sec\n"
     ]
    }
   ],
   "source": [
    "predictors = ['Canal_ID', 'Log_Target_mean_lag1', 'Log_Target_mean_lag2', 'Log_Target_mean_lag3', 'Log_Target_mean_lag4', \n",
    "              'Log_Target_mean_lag5','Lags_sum', 'brand', 'prodtype_cluster', 'Qty_Ruta_SAK_Bin', 'ZipCode', 'Producto_ID_clust_ID']\n",
    "\n",
    "\n",
    "model = xgb.XGBRegressor(n_estimators = 50, objective=\"reg:linear\", learning_rate= 0.1, max_depth=10,\n",
    "                         subsample=0.85,colsample_bytree=0.7)\n",
    "\n",
    "tic()\n",
    "dt, dv, dte = clusters_fit(model, train, val, test, predictors, target, IDcol)\n",
    "report_submit(dt, dv, dte, 'alg10.csv')\n",
    "tac()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's compare it with training the complete training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "RMSLE TRAIN:  0.4540303657\n",
      "RMSLE VAL:  0.46622409805\n",
      "NUM ROWS PREDICTED:  6999251\n",
      "NUM NEGATIVES PREDICTED:  0\n",
      "MIN TARGET PREDICTED:  0.0\n",
      "MEAN TARGET PREDICTED:  5.30053186416626\n",
      "MAX TARGET PREDICTED:  3500.06\n",
      "Time passed: 0hour:51min:45sec\n"
     ]
    }
   ],
   "source": [
    "predictors = ['Canal_ID', 'Log_Target_mean_lag1', 'Log_Target_mean_lag2', 'Log_Target_mean_lag3', 'Log_Target_mean_lag4', \n",
    "              'Log_Target_mean_lag5','Lags_sum', 'brand', 'prodtype_cluster', 'Qty_Ruta_SAK_Bin', 'ZipCode', 'Producto_ID_clust_ID']\n",
    "\n",
    "\n",
    "model = xgb.XGBRegressor(n_estimators = 500, objective=\"reg:linear\", learning_rate= 0.1, max_depth=10,\n",
    "                         subsample=0.85,colsample_bytree=0.7)\n",
    "\n",
    "tic()\n",
    "dt, dv, dte = model_fit(model, train, val, test, predictors, target, IDcol)\n",
    "report_submit(dt, dv, dte, 'alg10.2.csv')\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
