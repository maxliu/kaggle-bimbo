{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Building in XGBoost\n",
    "\n",
    "This is a great article for tunning XGboost: http://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "windows=False\n",
    "if (windows):\n",
    "    mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-5.3.0-posix-seh-rt_v4-rev0\\\\mingw64\\\\bin'\n",
    "    os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import boto # to download from AWS S3 buckets\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "import math\n",
    "\n",
    "_start_time = time.time()\n",
    "\n",
    "def tic():\n",
    "    global _start_time \n",
    "    _start_time = time.time()\n",
    "\n",
    "def tac():\n",
    "    t_sec = round(time.time() - _start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60) \n",
    "    print('Time passed: {}hour:{}min:{}sec'.format(t_hour,t_min,t_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's define variables that will define the behaviour of the whole script\n",
    "s3_path = 'http://bbts-kaggle.s3.amazonaws.com/bimbo/Pablo/'\n",
    "use_validation=True\n",
    "scale_numericals=True\n",
    "onehot_categoricals=False\n",
    "lag = 4\n",
    "num_clusters_cliente = 4000\n",
    "shifted_target = True\n",
    "trimmed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading File: train_modified_holdout_scaled_shifted_4lags_4000clusters.csv  ...\n",
      "Downloading File: val_modified_holdout_scaled_shifted_4lags_4000clusters.csv  ...\n",
      "Downloading File: test_modified_holdout_scaled_shifted_4lags_4000clusters.csv  ...\n",
      "Time passed: 0hour:4min:46sec\n"
     ]
    }
   ],
   "source": [
    "#now we load our modified train and test set\n",
    "tic()\n",
    "sufix=\"\"\n",
    "if (use_validation): sufix += \"_holdout\"\n",
    "if (scale_numericals): sufix += \"_scaled\"\n",
    "if (onehot_categoricals): sufix += \"_onehot\"\n",
    "if (shifted_target): sufix += \"_shifted\"\n",
    "sufix += \"_\" + repr(lag) + \"lags\"\n",
    "sufix += \"_\" + repr(num_clusters_cliente) + \"clusters\"\n",
    "\n",
    "print ('Downloading File: train_modified{}.csv  ...'.format(sufix))\n",
    "train = pd.read_csv(\"{}train_modified{}.csv\".format(s3_path,sufix),\n",
    "                    dtype = {'Canal_ID': 'int8',\n",
    "                            'log_target':  'float64',\n",
    "                            'Log_Target_mean_lag1': 'float64',\n",
    "                            'Log_Target_mean_lag2': 'float64',\n",
    "                            'Log_Target_mean_lag3': 'float64',\n",
    "                            'Log_Target_mean_lag4': 'float64',\n",
    "                            'Log_Target_mean_lag5': 'float64',\n",
    "                            'Lags_sum': 'float64',\n",
    "                            'pairs_mean':  'float64',\n",
    "                            'brand': 'int8',\n",
    "                            'prodtype_cluster': 'int32',\n",
    "                            'Qty_Ruta_SAK_Bin': 'int32',\n",
    "                            'ZipCode': 'uint32',\n",
    "                            'week_ct': 'int8',\n",
    "                            'NombreCliente': 'int32',\n",
    "                            'Producto_ID_clust_ID':'int32',\n",
    "                            'Ruta_SAK_clust_ID':'int32',\n",
    "                            'Agencia_ID_clust_ID':'int32',\n",
    "                            'Cliente_ID_clust_ID':'int32'},\n",
    "                   )\n",
    "\n",
    "print ('Downloading File: val_modified{}.csv  ...'.format(sufix))\n",
    "val = pd.read_csv(\"{}val_modified{}.csv\".format(s3_path,sufix),\n",
    "                    dtype = {'Canal_ID': 'int8',\n",
    "                            'log_target':  'float64',\n",
    "                            'Log_Target_mean_lag1': 'float64',\n",
    "                            'Log_Target_mean_lag2': 'float64',\n",
    "                            'Log_Target_mean_lag3': 'float64',\n",
    "                            'Log_Target_mean_lag4': 'float64',\n",
    "                            'Log_Target_mean_lag5': 'float64',\n",
    "                            'Lags_sum': 'float64',\n",
    "                            'pairs_mean':  'float64',\n",
    "                            'brand': 'int8',\n",
    "                            'prodtype_cluster': 'int32',\n",
    "                            'Qty_Ruta_SAK_Bin': 'int32',\n",
    "                            'ZipCode': 'uint32',\n",
    "                            'week_ct': 'int8',\n",
    "                            'NombreCliente': 'int32',\n",
    "                            'Producto_ID_clust_ID':'int32',\n",
    "                            'Ruta_SAK_clust_ID':'int32',\n",
    "                            'Agencia_ID_clust_ID':'int32',\n",
    "                            'Cliente_ID_clust_ID':'int32'},\n",
    "                   ) \n",
    "\n",
    "print ('Downloading File: test_modified{}.csv  ...'.format(sufix))\n",
    "test = pd.read_csv(\"{}test_modified{}.csv\".format(s3_path,sufix),\n",
    "                    dtype = {'id': 'uint32',\n",
    "                            'Canal_ID': 'int8',\n",
    "                            'Log_Target_mean_lag1': 'float64',\n",
    "                            'Log_Target_mean_lag2': 'float64',\n",
    "                            'Log_Target_mean_lag3': 'float64',\n",
    "                            'Log_Target_mean_lag4': 'float64',\n",
    "                            'Log_Target_mean_lag5': 'float64',\n",
    "                            'Lags_sum': 'float64',\n",
    "                            'pairs_mean':  'float64',\n",
    "                            'brand': 'int8',\n",
    "                            'prodtype_cluster': 'int32',\n",
    "                            'Qty_Ruta_SAK_Bin': 'int32',\n",
    "                            'ZipCode': 'uint32',\n",
    "                            'week_ct': 'int8',\n",
    "                            'NombreCliente': 'int32',\n",
    "                            'Producto_ID_clust_ID':'int32',\n",
    "                            'Ruta_SAK_clust_ID':'int32',\n",
    "                            'Agencia_ID_clust_ID':'int32',\n",
    "                            'Cliente_ID_clust_ID':'int32'},\n",
    "                      )\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define target and ID columns:\n",
    "target = 'log_target'\n",
    "IDcol = 'id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train multiple models per client cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we said on our prior step (Models wiht scikit-learn) that we need to deal with the data set high variance. Let's do this first:\n",
    "\n",
    "Looking at the plot below, created on the clustering-by-demand on the feature engineering notebook, we see that some client clusters behave very differntly from others. So this explain why our model is failing on predicting accurately for all of them.\n",
    "We are going then to create a wrapper function to create as many models as Client Clusters by demand are (Cliente_ID_clust_ID). The scores should be bettter individually, and the concatenation of all 400 models should yield a better overall RSMLE than our baseline 0.47.\n",
    "\n",
    "![Image of Variables vs Hypothesis](./input-data/h2o-clustByDem_Cliente_ID_400.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "def model_fit(alg, ctrain, cval, ctest, predictors, target, IDcol):\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    watchlist = [(cval[predictors], cval[target])]\n",
    "    alg.fit(ctrain[predictors], ctrain[target], eval_set=watchlist, eval_metric='rmse', early_stopping_rounds=20, verbose=False)\n",
    "\n",
    "\n",
    "    #Predict training set:\n",
    "    ctrain[\"predictions\"] = alg.predict(ctrain[predictors])\n",
    "    ctrain[\"predictions\"] = np.maximum(ctrain[\"predictions\"], 0)\n",
    "\n",
    "    \n",
    "    #Predict validation (holdout) set:\n",
    "    cval[\"predictions\"] = alg.predict(cval[predictors])\n",
    "    cval[\"predictions\"] = np.maximum(cval[\"predictions\"], 0)# we make all negative numbers = 0 since there cannot be a negative demand\n",
    "\n",
    "    \n",
    "    #Predict on testing data: we need to revert it back to target by applying expm1\n",
    "    ctest[target] = alg.predict(ctest[predictors])\n",
    "    ctest[target] = np.maximum(ctest[target], 0) # we make all negative numbers = 0 since there cannot be a negative demand\n",
    "    \n",
    "    return ctrain[[target,\"predictions\",\"Demanda_uni_equil\"]], cval[[target,\"predictions\",\"Demanda_uni_equil\"]], ctest[[IDcol,target]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clusters_fit (alg, dtrain, dval, dtest, predictors, target, IDcol):\n",
    "    \n",
    "    train_predictions = pd.DataFrame(index=[target,\"predictions\",\"Demanda_uni_equil\"])\n",
    "    val_predictions = pd.DataFrame(index=[target,\"predictions\",\"Demanda_uni_equil\"])\n",
    "    test_predictions = pd.DataFrame(index=[IDcol,target])\n",
    "    \n",
    "    clusters_list = train.Cliente_ID_clust_ID.drop_duplicates().get_values()\n",
    "    \n",
    "    for cluster in clusters_list:\n",
    "        \n",
    "        #we get the cluster train,val, test data\n",
    "\n",
    "        ctrain = dtrain.loc[dtrain[\"Cliente_ID_clust_ID\"] == cluster]\n",
    "        cval   = dval.loc[dval[\"Cliente_ID_clust_ID\"] == cluster]\n",
    "        ctest  = dtest.loc[dtest[\"Cliente_ID_clust_ID\"] == cluster]\n",
    "        \n",
    "        #we train the cluster\n",
    "        ctrain, cval, ctest = model_fit(model, ctrain, cval, ctest, predictors, target, IDcol)\n",
    "        \n",
    "        #rsmle_train =  np.sqrt(metrics.mean_squared_error(ctrain[target], ctrain[\"predictions\"]))\n",
    "        #rsmle_val = np.sqrt(metrics.mean_squared_error(cval[target], cval[\"predictions\"]))\n",
    "            \n",
    "        #concatenate each cluster result\n",
    "        train_predictions = pd.concat([train_predictions,ctrain],ignore_index=True)\n",
    "        val_predictions = pd.concat([val_predictions,cval],ignore_index=True)\n",
    "        test_predictions = pd.concat([test_predictions,ctest],ignore_index=True)\n",
    "        \n",
    "        #train_predictions.dropna(axis=0, how='any',inplace=True)\n",
    "        #val_predictions.dropna(axis=0, how='any',inplace=True)\n",
    "        #test_predictions.dropna(axis=0, how='any',inplace=True)\n",
    "          \n",
    "        #acc_rsmle_train =  np.sqrt(metrics.mean_squared_error(train_predictions[target], train_predictions[\"predictions\"]))\n",
    "        #acc_rsmle_val = np.sqrt(metrics.mean_squared_error(val_predictions[target], val_predictions[\"predictions\"]))\n",
    "        #rows_pct = cval.shape[0]*100/dval.shape[0]\n",
    "\n",
    "        #print('Cluster: {:.0f}  RMSLE T: {:.4f} RMSLE V: {:.4f}  RowsPct: {:.4f} - ACC. RSMLE TRAIN: {:.4f} - ACC. RSMLE VAL: {:.4f}'.format(\n",
    "        #      cluster, rsmle_train, rsmle_val ,rows_pct, acc_rsmle_train, acc_rsmle_val))\n",
    "    \n",
    "        \n",
    "    #For some reason this function is adding to NaN rows at the beggining, I don't know why, but we'll remove them\n",
    "    train_predictions.dropna(axis=0, how='any',inplace=True)\n",
    "    val_predictions.dropna(axis=0, how='any',inplace=True)\n",
    "    test_predictions.dropna(axis=0, how='any',inplace=True)\n",
    "    \n",
    "    return train_predictions, val_predictions, test_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_submit(dtrain, dval, dtest, filename):\n",
    "    \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    if(shifted_target):\n",
    "        print ('RMSLE TRAIN: ', np.sqrt(metrics.mean_squared_error(np.log1p(dtrain[\"Demanda_uni_equil\"]), dtrain[\"predictions\"])))\n",
    "        print ('RMSLE VAL: ', np.sqrt(metrics.mean_squared_error(np.log1p(dval[\"Demanda_uni_equil\"]), dval[\"predictions\"])))\n",
    "    else:\n",
    "        print ('RMSLE TRAIN: ', np.sqrt(metrics.mean_squared_error(dtrain[target], dtrain[\"predictions\"])))\n",
    "        print ('RMSLE VAL: ', np.sqrt(metrics.mean_squared_error(dval[target], dval[\"predictions\"])))\n",
    "    \n",
    "    #Predict on testing data: we need to revert it back to target by applying expm1\n",
    "    dtest[target] = np.expm1(dtest[target])\n",
    "    dtest[target] = np.maximum(dtest[target], 0) # we make all negative numbers = 0 since there cannot be a negative demand\n",
    "  \n",
    "    \n",
    "    print ('NUM ROWS PREDICTED: ', dtest.shape[0] )\n",
    "    print ('NUM NEGATIVES PREDICTED: ', dtest[target][dtest[target] < 0].count())\n",
    "    print ('MIN TARGET PREDICTED: ', dtest[target].min())\n",
    "    print ('MEAN TARGET PREDICTED: ', dtest[target].mean())\n",
    "    print ('MAX TARGET PREDICTED: ', dtest[target].max())\n",
    "    \n",
    "    #Export submission file:\n",
    "    submission = dtest.copy()\n",
    "    submission[IDcol] = submission[IDcol].astype(int)\n",
    "    submission.rename(columns={target: 'Demanda_uni_equil'}, inplace=True)\n",
    "    submission.to_csv(\"./Submissions/\"+filename, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alg6 - XGB - Train each client cluster separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training each of the client clusters separately and see if we have good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = ['Canal_ID', 'Log_Target_mean_lag1', 'Log_Target_mean_lag2', 'Log_Target_mean_lag3', 'Log_Target_mean_lag4', \n",
    "              'Agencia_ID','Ruta_SAK','Cliente_ID','Producto_ID',\n",
    "              'Lags_sum', 'brand', 'prodtype_cluster', 'Qty_Ruta_SAK_Bin', 'ZipCode', 'Producto_ID_clust_ID']\n",
    "\n",
    "\n",
    "model = xgb.XGBRegressor(n_estimators = 50, objective=\"reg:linear\", learning_rate= 0.1, max_depth=10,\n",
    "                         subsample=0.85,colsample_bytree=0.7)\n",
    "\n",
    "tic()\n",
    "dt, dv, dte = clusters_fit(model, train, val, test, predictors, target, IDcol)\n",
    "report_submit(dt, dv, dte, 'alg6.csv')\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>log_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>924190.0</td>\n",
       "      <td>1.479542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4521987.0</td>\n",
       "      <td>1.744488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6217476.0</td>\n",
       "      <td>1.315279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>970784.0</td>\n",
       "      <td>1.522702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3448837.0</td>\n",
       "      <td>2.396521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  log_target\n",
       "2   924190.0    1.479542\n",
       "3  4521987.0    1.744488\n",
       "4  6217476.0    1.315279\n",
       "5   970784.0    1.522702\n",
       "6  3448837.0    2.396521"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dte.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(val['log_target'], 100, alpha=0.5, label='target')\n",
    "plt.hist(val['predictions'], 100, alpha=0.5, label='predictions') \n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alg6.2 - XGB - Train only one batch\n",
    "\n",
    "And now let's compare it with training the complete training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = ['Canal_ID', 'Log_Target_mean_lag1', 'Log_Target_mean_lag2', 'Log_Target_mean_lag3', 'Log_Target_mean_lag4', \n",
    "              'Agencia_ID','Ruta_SAK','Cliente_ID','Producto_ID',\n",
    "              'Lags_sum', 'brand', 'prodtype_cluster', 'Qty_Ruta_SAK_Bin', 'ZipCode', 'Producto_ID_clust_ID']\n",
    "\n",
    "model = xgb.XGBRegressor(n_estimators = 300, objective=\"reg:linear\", learning_rate= 0.1, max_depth=10,\n",
    "                         subsample=0.85,colsample_bytree=0.7)\n",
    "\n",
    "tic()\n",
    "dt, dv, dte = model_fit(model, train, val, test, predictors, target, IDcol)\n",
    "report_submit(dt, dv, dte, 'alg6.2.csv')\n",
    "tac()\n",
    "\n",
    "feat_imp = pd.Series(model.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(model.evals_result()['validation_0']['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
