{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters by Demand\n",
    "by LMZintgraf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "\n",
    "_start_time = time.time()\n",
    "\n",
    "# define a easy timing function to use going forward\n",
    "def tic():\n",
    "    global _start_time \n",
    "    _start_time = time.time()\n",
    "\n",
    "def tac():\n",
    "    t_sec = round(time.time() - _start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60) \n",
    "    print('Time passed: {}hour:{}min:{}sec'.format(t_hour,t_min,t_sec))\n",
    "    \n",
    "# utility function- display large dataframes in an html iframe\n",
    "def df_display(df, lines=500):\n",
    "    txt = (\"<iframe \" +\n",
    "           \"srcdoc='\" + df.head(lines).to_html() + \"' \" +\n",
    "           \"width=1000 height=500>\" +\n",
    "           \"</iframe>\")\n",
    "\n",
    "    return IPython.display.HTML(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed: 0hour:1min:13sec\n"
     ]
    }
   ],
   "source": [
    "#Read files:\n",
    "tic()\n",
    "train = pd.read_csv('input-data/train.csv',\n",
    "                           dtype  = {'Semana': 'int8',\n",
    "                                     'Producto_ID':'int32',\n",
    "                                     'Cliente_ID':'int32',\n",
    "                                     'Agencia_ID':'uint16',\n",
    "                                     'Canal_ID':'int8',\n",
    "                                     'Ruta_SAK':'int32',\n",
    "                                     'Venta_hoy':'float32',\n",
    "                                     'Venta_uni_hoy': 'int8',\n",
    "                                     'Dev_uni_proxima':'int8',\n",
    "                                     'Dev_proxima':'float32',\n",
    "                                     'Demanda_uni_equil':'int32'})\n",
    "test = pd.read_csv('input-data/test.csv',\n",
    "                           dtype  = {'Semana': 'int8',\n",
    "                                     'Producto_ID':'int32',\n",
    "                                     'Cliente_ID':'int32',\n",
    "                                     'Agencia_ID':'uint16',\n",
    "                                     'Canal_ID':'int8',\n",
    "                                     'Ruta_SAK':'int32'})\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove unnecessary fields in training data\n",
    "train.drop(['Venta_uni_hoy', 'Venta_hoy','Dev_uni_proxima', 'Dev_proxima'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Since test dataframe is not the same as train dataframe, we make them equal by removing and adding columns\n",
    "train.insert(0, 'id', np.nan)\n",
    "test.insert(7, 'Demanda_uni_equil', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed: 0hour:0min:17sec\n",
      "(74180464, 9) (6999251, 9) (81179715, 9)\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "train['source']='train'\n",
    "test['source']='test'\n",
    "data = pd.concat([train, test],ignore_index=True)\n",
    "tac()\n",
    "print (train.shape, test.shape, data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First thing we need to do is to transform our target ( Demanda_uni_equil) to log(1 + demand) - this makes sense since we're \n",
    "#trying to minimize rmsle vs the mean which minimizes rmse. At the end of the modeling (for submission) we need to reverse it \n",
    "#by applying expm1(x)\n",
    "\n",
    "data['log_target'] = np.log1p(data[\"Demanda_uni_equil\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed: 0hour:1min:13sec\n"
     ]
    }
   ],
   "source": [
    "#Let's also create all the grouping dataframes we are going to need \n",
    "tic()\n",
    "\n",
    "global_mean = data['log_target'].mean()\n",
    "prod_mean = data.groupby('Producto_ID').agg({'log_target': 'mean' })\n",
    "client_mean = data.groupby('Cliente_ID').agg({'log_target': 'mean' })\n",
    "prod_client_mean = data.groupby(['Producto_ID', 'Cliente_ID']).agg({'log_target': 'mean' })\n",
    "\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed: 0hour:0min:29sec\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "prod_mean_dict = prod_mean.to_dict()\n",
    "prod_client_mean_dict = prod_client_mean.to_dict()\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_pairs_mean_feature(key):\n",
    "    key = tuple(key)\n",
    "    product = key[0]\n",
    "    client = key[1]\n",
    "    \n",
    "    val = prod_client_mean_dict['log_target'][(product,client)]\n",
    "    if np.isnan(val):\n",
    "        val = prod_mean_dict['log_target'][(product)]\n",
    "        if np.isnan(val):\n",
    "            val = client_mean_dict['log_target'][(client)]\n",
    "            \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tic()\n",
    "data['pairs_mean'] = data[['Producto_ID', 'Cliente_ID']].apply(lambda x:gen_pairs_mean_feature(x), axis=1)\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Cluster features by demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- HYPERPARAMETERS FOR FEATURE EXTRACTION ---\n",
    "\n",
    "# number of clusters to group depot/route/produc (if 0 will not be added as feature)\n",
    "num_clusters_agencia = [25]\n",
    "num_clusters_ruta = [100]\n",
    "num_clusters_cliente = [150]\n",
    "num_clusters_producto = [150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to make data categorical\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# for clustering\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to find the right amount of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist, pdist\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Determine your k range\n",
    "k_range = range(1,14)\n",
    "\n",
    "# Fit the kmeans model for each n_clusters = k\n",
    "k_means_var = [KMeans(n_clusters=k).fit(hpc) for k in k_range]\n",
    "\n",
    "# Pull out the cluster centers for each model\n",
    "centroids = [X.cluster_centers_ for X in k_means_var]\n",
    "\n",
    "# Calculate the Euclidean distance from \n",
    "# each point to each cluster center\n",
    "k_euclid = [cdist(hpc, cent, 'euclidean') for cent in centroids]\n",
    "dist = [np.min(ke,axis=1) for ke in k_euclid]\n",
    "\n",
    "# Total within-cluster sum of squares\n",
    "wcss = [sum(d**2) for d in dist]\n",
    "\n",
    "# The total sum of squares\n",
    "tss = sum(pdist(hpc)**2)/hpc.shape[0]\n",
    "\n",
    "# The between-cluster sum of squares\n",
    "bss = tss - wcss\n",
    "\n",
    "# elbow curve\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(k_range, bss/tss*100, 'b*-')\n",
    "ax.set_ylim((0,100))\n",
    "plt.grid(True)\n",
    "plt.xlabel('n_clusters')\n",
    "plt.ylabel('Percentage of variance explained')\n",
    "plt.title('Variance Explained vs. k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clusters_by_demand(feats, tars, num_clusters, feat_name):\n",
    "    '''\n",
    "    For a given feature vector, respective target vector, and number of clusters, returns\n",
    "        - feats_unique: a list of unique feature values\n",
    "        - clusters: for each unique feature, the cluster (ID) it belongs to\n",
    "        - demand_info_unique: for each unique feature, some statistics about the demand for that feature\n",
    "    '''\n",
    "    # get the unique features which we then want to cluster\n",
    "    feats_unique = np.unique(feats)\n",
    "    \n",
    "    # for each unique feature, we will get the median/std demand\n",
    "    demand_info_unique = np.zeros((len(feats_unique),2))\n",
    "    for f in range(len(feats_unique)):\n",
    "        all_orders = tars[feats==feats_unique[f]]\n",
    "        demand_info_unique[f] = [np.median(all_orders), np.std(all_orders)]\n",
    "        \n",
    "    demand_info_unique[np.isnan(demand_info_unique)] = 0\n",
    "        \n",
    "    # we use the kmeans clustering algorithm\n",
    "    kmeans = KMeans(num_clusters, n_jobs=-1)\n",
    "    clusters = kmeans.fit_predict(demand_info_unique)\n",
    "    \n",
    "    # plot demand/cluster\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for c in range(num_clusters):\n",
    "        d_median = demand_info_unique[clusters==c,0]\n",
    "        d_std = demand_info_unique[clusters==c,1]\n",
    "        plt.plot(d_median,d_std,'.')\n",
    "        plt.xlabel('median')\n",
    "        plt.ylabel('std')\n",
    "    plt.savefig(path_datadrive+'clustByDem_{}_{}'.format(feat_name, num_clusters))\n",
    "\n",
    "    # return a mapping from unique feature values to clusters, and the demand info per cluster\n",
    "    return feats_unique, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_and_save(feat_name, num_clusters):\n",
    "    '''\n",
    "    Input: \n",
    "        - idx: the index of the feature we want to cluster, one-hot-encode, and add to our features\n",
    "        - num_clusters: the number of clusters we want to use to group the feature values\n",
    "    '''\n",
    "    global data\n",
    "    \n",
    "    # run clustering by demand using info from week 3-9\n",
    "    feats_train = data[feat_name][data[\"Semana\"]<9].get_values()\n",
    "    tars_train = data['pairs_mean'][data['Semana']<9].get_values()\n",
    "    feats_unique_train, clusters_train = get_clusters_by_demand(feats_train, tars_train, num_clusters, feat_name)\n",
    "    \n",
    "    # create new dataframe to save the mapping from feature ID to cluster ID\n",
    "    a = np.hstack((feats_unique_train[:,np.newaxis], clusters_train[:,np.newaxis]))\n",
    "    feat_clust_map = pd.DataFrame(data = a, columns=[feat_name,feat_name+'_clust_ID'])\n",
    "\n",
    "    # save the new feature\n",
    "    feat_clust_map.to_csv(path_datadrive+\"clustByDem_{}_{}\".format(feat_name, num_clusters), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in num_clusters_agencia:\n",
    "    print(\"agencia...\", c)\n",
    "    cluster_and_save('Agencia_ID', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in num_clusters_ruta:\n",
    "    print(\"ruta... \", c)\n",
    "    cluster_and_save('Ruta_SAK', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in num_clusters_cliente:\n",
    "    print(\"client...\", c)\n",
    "    cluster_and_save('Cliente_ID', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in num_clusters_producto:\n",
    "    print(\"prod...\", c)\n",
    "    cluster_and_save('Producto_ID', c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
