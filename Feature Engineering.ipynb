{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "I will explore the problem in following stages:\n",
    "\n",
    "1.  **Hypothesis Generation** – understanding the problem better by brainstorming possible factors that can impact the outcome\n",
    "2.  **Data Exploration** – looking at categorical and continuous feature summaries and making inferences about the data.\n",
    "3.  **Data Cleaning** – imputing missing values in the data and checking for outliers\n",
    "4.  **Feature Engineering** – modifying existing variables and creating new ones for analysis\n",
    "5.  **Model Building** – making predictive models on the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hypothesis Generation\n",
    "\n",
    "This is a very pivotal step in the process of analyzing data. This involves understanding the problem and making some hypothesis about what could potentially have a good impact on the outcome. This is done BEFORE looking at the data, and we end up creating a laundry list of the different analysis which we can potentially perform if data is available.\n",
    "\n",
    "### The Problem Statement\n",
    "\n",
    "Understanding the problem statement is the first and foremost step:\n",
    "\n",
    "> In this competition, you will forecast the demand of a product for a given week, at a particular store. The dataset you are given consists of 9 weeks of sales transactions in Mexico. Every week, there are delivery trucks that deliver products to the vendors. Each transaction consists of sales and returns. Returns are the products that are unsold and expired. The demand for a product in a certain week is defined as the sales this week subtracted by the return next week.\n",
    "\n",
    "So the idea is to find out the demand of a product (sales - returns) per client, and store which impacts the sales of a product. Let’s think about some of the analysis that can be done and come up with certain hypothesis.\n",
    "\n",
    "### The Hypotheses\n",
    "\n",
    "I came up with the following hypothesis while thinking about the problem. Since we’re talking about stores and products, lets make different sets for each.\n",
    "\n",
    "**Store/Client Level Hypotheses:**\n",
    "\n",
    "1.  **Town type:** Stores located in urban or Tier 1 towns should have higher sales because of the higher income levels of people there.\n",
    "2.  **Population Density:** Stores located in densely populated areas should have higher sales because of more demand.\n",
    "3.  **Store Capacity:** Stores which are very big in size should have higher sales as they act like one-stop-shops and people would prefer getting everything from one place\n",
    "4.  **Competitors:** Stores having similar establishments nearby should have less sales because of more competition.\n",
    "5.  **Marketing:** Stores which have a good marketing division should have higher sales as it will be able to attract customers through the right offers and advertising.\n",
    "6.  **Location:** Stores located within popular marketplaces should have higher sales because of better access to customers.\n",
    "7.  **Customer Behavior:** Stores keeping the right set of products to meet the local needs of customers will have higher sales.\n",
    "8.  **Ambiance:** Stores which are well-maintained and managed by polite and humble people are expected to have higher footfall and thus higher sales.\n",
    "9.  **Season:** Store should sell more after customer's pay day: after 15th or 30th of the month\n",
    "\n",
    "**Product Level Hypotheses:**\n",
    "\n",
    "1.  **Brand:** Branded products should have higher sales because of higher trust in the customer.\n",
    "2.  **Packaging:** Products with good packaging can attract customers and sell more.\n",
    "3.  **Utility:** Daily use products should have a higher tendency to sell as compared to the specific use products.\n",
    "4.  **Display Area:** Products which are given bigger shelves in the store are likely to catch attention first and sell more.\n",
    "5.  **Visibility in Store:** The location of product in a store will impact sales. Ones which are right at entrance will catch the eye of customer first rather than the ones in back.\n",
    "6.  **Advertising:** Better advertising of products in the store will should higher sales in most cases.\n",
    "7.  **Promotional Offers:** Products accompanied with attractive offers and discounts will sell more.\n",
    "\n",
    "\n",
    "Lets move on to the data exploration where we will have a look at the data in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\\. Data Exploration\n",
    "\n",
    "We’ll be performing some basic data exploration here and come up with some inferences about the data.\n",
    "\n",
    "The first step is to look at the data and try to identify the information which we hypothesized vs the available data. A comparison between the data dictionary on the competition page and out hypotheses is shown below:\n",
    "\n",
    "![Image of Variables vs Hypothesis](files/../input-data/Variables_vs_Hyphotesis.png)\n",
    "\n",
    "We can summarize the findings as:\n",
    "\n",
    "** 9 Features Hypothesized but not found in actual data. **\n",
    "\n",
    "** 5 Features Hypothesized as well as present in the data **\n",
    "\n",
    "** 3 Features present in the data but not hypothesized. **\n",
    "\n",
    "\n",
    "We invariable find features which we hypothesized, but data doesn’t carry and vice versa. We should look for open source data to fill the gaps if possible. Let’s start by loading the required libraries and data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "\n",
    "_start_time = time.time()\n",
    "\n",
    "def tic():\n",
    "    global _start_time \n",
    "    _start_time = time.time()\n",
    "\n",
    "def tac():\n",
    "    t_sec = round(time.time() - _start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60) \n",
    "    print('Time passed: {}hour:{}min:{}sec'.format(t_hour,t_min,t_sec))\n",
    "    \n",
    "# display large dataframes in an html iframe\n",
    "def df_display(df, lines=500):\n",
    "    txt = (\"<iframe \" +\n",
    "           \"srcdoc='\" + df.head(lines).to_html() + \"' \" +\n",
    "           \"width=1000 height=500>\" +\n",
    "           \"</iframe>\")\n",
    "\n",
    "    return IPython.display.HTML(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed: 0hour:1min:27sec\n"
     ]
    }
   ],
   "source": [
    "#Read files:\n",
    "tic()\n",
    "train = pd.read_csv('input-data/train.csv',\n",
    "                           dtype  = {'Semana': 'int8',\n",
    "                                     'Producto_ID':'int32',\n",
    "                                     'Cliente_ID':'int32',\n",
    "                                     'Agencia_ID':'uint16',\n",
    "                                     'Canal_ID':'int8',\n",
    "                                     'Ruta_SAK':'int32',\n",
    "                                     'Venta_hoy':'float32',\n",
    "                                     'Venta_uni_hoy': 'int8',\n",
    "                                     'Dev_uni_proxima':'int8',\n",
    "                                     'Dev_proxima':'float32',\n",
    "                                     'Demanda_uni_equil':'int32'})\n",
    "test = pd.read_csv('input-data/test.csv',\n",
    "                           dtype  = {'Semana': 'int8',\n",
    "                                     'Producto_ID':'int32',\n",
    "                                     'Cliente_ID':'int32',\n",
    "                                     'Agencia_ID':'uint16',\n",
    "                                     'Canal_ID':'int8',\n",
    "                                     'Ruta_SAK':'int32'})\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Semana</th>\n",
       "      <th>Agencia_ID</th>\n",
       "      <th>Canal_ID</th>\n",
       "      <th>Ruta_SAK</th>\n",
       "      <th>Cliente_ID</th>\n",
       "      <th>Producto_ID</th>\n",
       "      <th>Venta_uni_hoy</th>\n",
       "      <th>Venta_hoy</th>\n",
       "      <th>Dev_uni_proxima</th>\n",
       "      <th>Dev_proxima</th>\n",
       "      <th>Demanda_uni_equil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>3301</td>\n",
       "      <td>15766</td>\n",
       "      <td>1212</td>\n",
       "      <td>3</td>\n",
       "      <td>25.139999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>3301</td>\n",
       "      <td>15766</td>\n",
       "      <td>1216</td>\n",
       "      <td>4</td>\n",
       "      <td>33.520000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>3301</td>\n",
       "      <td>15766</td>\n",
       "      <td>1238</td>\n",
       "      <td>4</td>\n",
       "      <td>39.320000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>3301</td>\n",
       "      <td>15766</td>\n",
       "      <td>1240</td>\n",
       "      <td>4</td>\n",
       "      <td>33.520000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>3301</td>\n",
       "      <td>15766</td>\n",
       "      <td>1242</td>\n",
       "      <td>3</td>\n",
       "      <td>22.920000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Semana  Agencia_ID  Canal_ID  Ruta_SAK  Cliente_ID  Producto_ID  \\\n",
       "0       3        1110         7      3301       15766         1212   \n",
       "1       3        1110         7      3301       15766         1216   \n",
       "2       3        1110         7      3301       15766         1238   \n",
       "3       3        1110         7      3301       15766         1240   \n",
       "4       3        1110         7      3301       15766         1242   \n",
       "\n",
       "   Venta_uni_hoy  Venta_hoy  Dev_uni_proxima  Dev_proxima  Demanda_uni_equil  \n",
       "0              3  25.139999                0            0                  3  \n",
       "1              4  33.520000                0            0                  4  \n",
       "2              4  39.320000                0            0                  4  \n",
       "3              4  33.520000                0            0                  4  \n",
       "4              3  22.920000                0            0                  3  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Semana</th>\n",
       "      <th>Agencia_ID</th>\n",
       "      <th>Canal_ID</th>\n",
       "      <th>Ruta_SAK</th>\n",
       "      <th>Cliente_ID</th>\n",
       "      <th>Producto_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4037</td>\n",
       "      <td>1</td>\n",
       "      <td>2209</td>\n",
       "      <td>4639078</td>\n",
       "      <td>35305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2237</td>\n",
       "      <td>1</td>\n",
       "      <td>1226</td>\n",
       "      <td>4705135</td>\n",
       "      <td>1238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2045</td>\n",
       "      <td>1</td>\n",
       "      <td>2831</td>\n",
       "      <td>4549769</td>\n",
       "      <td>32940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1227</td>\n",
       "      <td>1</td>\n",
       "      <td>4448</td>\n",
       "      <td>4717855</td>\n",
       "      <td>43066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1219</td>\n",
       "      <td>1</td>\n",
       "      <td>1130</td>\n",
       "      <td>966351</td>\n",
       "      <td>1277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Semana  Agencia_ID  Canal_ID  Ruta_SAK  Cliente_ID  Producto_ID\n",
       "0   0      11        4037         1      2209     4639078        35305\n",
       "1   1      11        2237         1      1226     4705135         1238\n",
       "2   2      10        2045         1      2831     4549769        32940\n",
       "3   3      11        1227         1      4448     4717855        43066\n",
       "4   4      11        1219         1      1130      966351         1277"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Since test dataframe is not the same as train dataframe, we make them equal by removing and adding columns\n",
    "train.insert(0, 'id', np.nan)\n",
    "test.insert(7, 'Venta_uni_hoy', np.nan)\n",
    "test.insert(8, 'Venta_hoy', np.nan)\n",
    "test.insert(9, 'Dev_uni_proxima', np.nan)\n",
    "test.insert(10, 'Dev_proxima', np.nan)\n",
    "test.insert(11, 'Demanda_uni_equil', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Semana</th>\n",
       "      <th>Agencia_ID</th>\n",
       "      <th>Canal_ID</th>\n",
       "      <th>Ruta_SAK</th>\n",
       "      <th>Cliente_ID</th>\n",
       "      <th>Producto_ID</th>\n",
       "      <th>Venta_uni_hoy</th>\n",
       "      <th>Venta_hoy</th>\n",
       "      <th>Dev_uni_proxima</th>\n",
       "      <th>Dev_proxima</th>\n",
       "      <th>Demanda_uni_equil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>3301</td>\n",
       "      <td>15766</td>\n",
       "      <td>1212</td>\n",
       "      <td>3</td>\n",
       "      <td>25.139999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>3301</td>\n",
       "      <td>15766</td>\n",
       "      <td>1216</td>\n",
       "      <td>4</td>\n",
       "      <td>33.520000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>3301</td>\n",
       "      <td>15766</td>\n",
       "      <td>1238</td>\n",
       "      <td>4</td>\n",
       "      <td>39.320000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>3301</td>\n",
       "      <td>15766</td>\n",
       "      <td>1240</td>\n",
       "      <td>4</td>\n",
       "      <td>33.520000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>3301</td>\n",
       "      <td>15766</td>\n",
       "      <td>1242</td>\n",
       "      <td>3</td>\n",
       "      <td>22.920000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Semana  Agencia_ID  Canal_ID  Ruta_SAK  Cliente_ID  Producto_ID  \\\n",
       "0 NaN       3        1110         7      3301       15766         1212   \n",
       "1 NaN       3        1110         7      3301       15766         1216   \n",
       "2 NaN       3        1110         7      3301       15766         1238   \n",
       "3 NaN       3        1110         7      3301       15766         1240   \n",
       "4 NaN       3        1110         7      3301       15766         1242   \n",
       "\n",
       "   Venta_uni_hoy  Venta_hoy  Dev_uni_proxima  Dev_proxima  Demanda_uni_equil  \n",
       "0              3  25.139999                0            0                  3  \n",
       "1              4  33.520000                0            0                  4  \n",
       "2              4  39.320000                0            0                  4  \n",
       "3              4  33.520000                0            0                  4  \n",
       "4              3  22.920000                0            0                  3  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Semana</th>\n",
       "      <th>Agencia_ID</th>\n",
       "      <th>Canal_ID</th>\n",
       "      <th>Ruta_SAK</th>\n",
       "      <th>Cliente_ID</th>\n",
       "      <th>Producto_ID</th>\n",
       "      <th>Venta_uni_hoy</th>\n",
       "      <th>Venta_hoy</th>\n",
       "      <th>Dev_uni_proxima</th>\n",
       "      <th>Dev_proxima</th>\n",
       "      <th>Demanda_uni_equil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4037</td>\n",
       "      <td>1</td>\n",
       "      <td>2209</td>\n",
       "      <td>4639078</td>\n",
       "      <td>35305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2237</td>\n",
       "      <td>1</td>\n",
       "      <td>1226</td>\n",
       "      <td>4705135</td>\n",
       "      <td>1238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2045</td>\n",
       "      <td>1</td>\n",
       "      <td>2831</td>\n",
       "      <td>4549769</td>\n",
       "      <td>32940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1227</td>\n",
       "      <td>1</td>\n",
       "      <td>4448</td>\n",
       "      <td>4717855</td>\n",
       "      <td>43066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1219</td>\n",
       "      <td>1</td>\n",
       "      <td>1130</td>\n",
       "      <td>966351</td>\n",
       "      <td>1277</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Semana  Agencia_ID  Canal_ID  Ruta_SAK  Cliente_ID  Producto_ID  \\\n",
       "0   0      11        4037         1      2209     4639078        35305   \n",
       "1   1      11        2237         1      1226     4705135         1238   \n",
       "2   2      10        2045         1      2831     4549769        32940   \n",
       "3   3      11        1227         1      4448     4717855        43066   \n",
       "4   4      11        1219         1      1130      966351         1277   \n",
       "\n",
       "   Venta_uni_hoy  Venta_hoy  Dev_uni_proxima  Dev_proxima  Demanda_uni_equil  \n",
       "0            NaN        NaN              NaN          NaN                NaN  \n",
       "1            NaN        NaN              NaN          NaN                NaN  \n",
       "2            NaN        NaN              NaN          NaN                NaN  \n",
       "3            NaN        NaN              NaN          NaN                NaN  \n",
       "4            NaN        NaN              NaN          NaN                NaN  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its generally a good idea to combine both train and test data sets into one, perform feature engineering and then divide them later again. This saves the trouble of performing the same steps twice on test and train. Lets combine them into a dataframe ‘data’ with a ‘source’ column specifying where each observation belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed: 0hour:4min:28sec\n",
      "(74180464, 13) (6999251, 13) (81179715, 13)\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "train['source']='train'\n",
    "test['source']='test'\n",
    "data = pd.concat([train, test],ignore_index=True)\n",
    "tac()\n",
    "print (train.shape, test.shape, data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can see that data has same #columns but rows equivalent to both test and train. Lets start by checking which columns contain missing values. (takes 32mins to run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.apply(lambda x: sum(x.isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be any missing values (other than the NaN we set on the test and train sets).\n",
    "\n",
    "Lets look at some basic statistics for numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some observations:\n",
    "\n",
    "   Looking at Demanda_uni_equil (our target), or the amount of product sold per week, we find interesting things:\n",
    "   \n",
    "   **1)** The average is 7.22, so in average there is 7 units per week per store sold.\n",
    "   \n",
    "   **2)** Looking at the max of 5000, it looks very far fro the mean (3 orders of magnitude), so we must check for an outlier here or a store that is crazy different from the rest.\n",
    "   \n",
    "   **3)** Same behaviour we find on Dev_uni_proxima, Venta_hoy and Venta_uni_hoy\n",
    "   \n",
    "Looking at the nice data analysis made in R by Faviens, here: https://www.kaggle.com/fabienvs/grupo-bimbo-inventory-demand/notebook-8a62eda039a3b0b944cf/notebook we corroborate the outlier(s):\n",
    "There is a massive client: Puebla Remision\n",
    "   \n",
    "![Image of size of Customers]( https://www.kaggle.io/svf/267812/783a24d1dd546819a44914f996b249e8/__results___files/figure-html/unnamed-chunk-16-1.png)\n",
    "   \n",
    "\n",
    "Moving to nominal (categorical) variable, lets have a look at the number of unique values in each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, in train and test sets, we have 552 Agencies(depots), 890k clients (we might have some repeated clients due to typos when enterind data), 1833 products (we might have some repeated products here based on typos) and 3620 routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see how many records we have per week\n",
    "for i in range(3,12):\n",
    "    print(\"Semana\"+repr(i)+\" =\\t\" + repr(data[data[\"Semana\"]==i].Semana.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\\. Data Cleaning\n",
    "\n",
    "This step typically involves imputing missing values and treating outliers. As we saw before, there are no missing values. Regarding outliers, there seem to be an obvious one, but we are going to see later on if its necessary to treat it differently.\n",
    "\n",
    "My initial reaction would be to see if anything with the word REMISION is on the test set. if not, then delete it. See this discussion: https://www.kaggle.com/c/grupo-bimbo-inventory-demand/forums/t/22037/puebla-remission/126053"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's find out who are the clients with the word REMISION on it\n",
    "client_name = pd.read_csv('files/../input-data/cliente_tabla.csv')\n",
    "cliente_id_name_train = pd.merge(train,client_name, on='Cliente_ID')\n",
    "cliente_id_name_test = pd.merge(test,client_name, on='Cliente_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cliente_id_name_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cliente_id_name_train[cliente_id_name_train.NombreCliente.str.contains('REMISION')].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the word \"REMISION\" shows up 140k times on the train set. Let's see the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cliente_id_name_test[cliente_id_name_test.NombreCliente.str.contains('REMISION')].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12k rows shows up the word REMISION on the test set. This implies that it has to be predicted as well. We cannot eliminate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\\. Feature Engineering\n",
    "\n",
    "We explored some nuances in the data in the data exploration section. Lets move on to resolving them and making our data ready for analysis. We will also create some new variables using the existing ones in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First thing we need to do is to transform our target ( Demanda_uni_equil) to log(1 + demand) - this makes sense since we're \n",
    "#trying to minimize rmsle and the mean minimizes rmse. At the end of the modeling (for submission) we need to reverse it \n",
    "#by applying expm1(x)\n",
    "\n",
    "data['log_target'] = np.log1p(data[\"Demanda_uni_equil\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's also create all the grouping dataframes we are going to need \n",
    "tic()\n",
    "\n",
    "global_mean = data['log_target'].mean()\n",
    "prod_mean = data.groupby('Producto_ID').agg({'log_target': 'mean' })\n",
    "client_mean = data.groupby('Cliente_ID').agg({'log_target': 'mean' })\n",
    "prod_client_mean = data.groupby(['Producto_ID', 'Cliente_ID']).agg({'log_target': 'mean' })\n",
    "semana_client_prod_mean = data.groupby(['Semana','Cliente_ID','Producto_ID']).agg({'log_target': 'mean'})\n",
    "\n",
    "tac()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature1: Lag Features - Demand per client-product pair for prior weeks\n",
    "Based on this blog: http://blog.nycdatascience.com/student-works/predicting-demand-historical-sales-data-grupo-bimbo-kaggle-competition/\n",
    "\n",
    "As this script said: https://www.kaggle.com/bpavlyshenko/grupo-bimbo-inventory-demand/bimbo-xgboost-r-script-lb-0-457/code\n",
    "It is important to know what were the previous weeks sales. If the previous week, too many products were supplied and they were not sold, the next week this product amount, supplied to the same store, will be decreased. So it is very important to included lag values of target variable as a feature to predict the next sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = semana_client_prod_mean.reset_index() # we convert the index to columns for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see how many records we have per week on the semana_cliente_Producto groups vs the raw dataset\n",
    "for i in range(3,12):\n",
    "    print(\"Semana\"+repr(i)+\" =\\t\" + repr(data[data[\"Semana\"]==i].Semana.count())+ \" (raw)\\t\" +\n",
    "            repr(df[df[\"Semana\"]==i].Semana.count()) + \" (group)\\t\" +  \n",
    "            repr(data[data[\"Semana\"]==i].Semana.count() - df[df[\"Semana\"]==i].Semana.count()) + \" (diff)\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, there are repeated combinations of client-product on each week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Before we start adding lags and removing rows, let's see the size of our dataset\n",
    "size_data = data.memory_usage().sum()\n",
    "print(size_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#here we add the number of lags we want\n",
    "tic()\n",
    "lag=4\n",
    "for i in range(1,lag+1):\n",
    "    df['Semana'] += 1\n",
    "    df.rename(columns={df.columns[3]: 'Log_Target_mean_lag%d' %(i)}, inplace=True)\n",
    "    data = pd.merge(data,df, how = 'left', on = ['Semana','Cliente_ID','Producto_ID'])\n",
    "    data['Log_Target_mean_lag%d' %(i)].fillna(0, inplace=True) # we replace the client-product log mean NaN/Not found on the week before with ZERO\n",
    "    data = data[data.Semana != i+2] # here we delete the week rows we dont have lags for\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's see how many NaN or Nulls we have\n",
    "data.apply(lambda x: sum(x.isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above looks correct! the only NaN shown are the variables that are not avaibable on the test set, everyting else looks good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see how many records we have per week and make sure we didn't delete any data from our important weeks\n",
    "for i in range(3,12):\n",
    "    print(\"Semana\"+repr(i)+\" =\\t\" + repr(data[data[\"Semana\"]==i].Semana.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now let's see how much was the data set size reduced/increased\n",
    "new_size_data = data.memory_usage().sum()\n",
    "print(\"Dataset size changed in \" + repr((new_size_data - size_data )*100/new_size_data) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature 2:  Calculates de sum of prior weeks Log mean Demands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We want to sum the lags up until week 9, this means that we need to sum lag2 and up.\n",
    "data['Lags_sum'] = 0\n",
    "for i in range(1,lag+1):\n",
    "    data['Lags_sum'] += data['Log_Target_mean_lag%d' %(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature 3:  Mean Demand per client-product pair - Product/Client demand, Product demand, Global demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic()\n",
    "\n",
    "prod_mean_dict = prod_mean.to_dict()\n",
    "prod_client_mean_dict = prod_client_mean.to_dict()\n",
    "\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_pairs_mean_feature(key):\n",
    "    key = tuple(key)\n",
    "    product = key[0]\n",
    "    client = key[1]\n",
    "    \n",
    "    val = prod_client_mean_dict['log_target'][(product,client)]\n",
    "    if np.isnan(val):\n",
    "        val = prod_mean_dict['log_target'][(product)]\n",
    "        if np.isnan(val):\n",
    "            val = global_mean\n",
    "            \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (global_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's see how many products are new (appear on week 10 and 11 but not on past weeks)\n",
    "prod_mean.apply(lambda x: sum(x.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's see how many combinations of products-clients are new (appear on week 10 and 11 but not on past weeks) = \n",
    "prod_client_mean.apply(lambda x: sum(x.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic()\n",
    "data['pairs_mean'] = data[['Producto_ID', 'Cliente_ID']].apply(lambda x:gen_pairs_mean_feature2(x), axis=1)\n",
    "tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 4: Create a broad category of Brand of item (brand hypothesis)\n",
    "Let's preprocess products a little bit. I borrowed some of the preprocessing from here: https://www.kaggle.com/vykhand/grupo-bimbo-inventory-demand/exploring-products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products  =  pd.read_csv(\"input-data/producto_tabla.csv\")\n",
    "products  =  pd.read_csv(\"input-data/producto_tabla.csv\")\n",
    "#products['short_name'] = products.NombreProducto.str.extract('^(\\D*)', expand=False)#python 2.7\n",
    "products['short_name'] = products.NombreProducto.str.extract('^(\\D*)')#python 3.0\n",
    "#products['brand'] = products.NombreProducto.str.extract('^.+\\s(\\D+) \\d+$', expand=False)\n",
    "products['brand'] = products.NombreProducto.str.extract('^.+\\s(\\D+) \\d+$')\n",
    "#w = products.NombreProducto.str.extract('(\\d+)(Kg|g)', expand=True)\n",
    "w = products.NombreProducto.str.extract('(\\d+)(Kg|g)')\n",
    "products['weight'] = w[0].astype('float')*w[1].map({'Kg':1000, 'g':1})\n",
    "#products['pieces'] =  products.NombreProducto.str.extract('(\\d+)p ', expand=False).astype('float')\n",
    "products['pieces'] =  products.NombreProducto.str.extract('(\\d+)p ').astype('float')\n",
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products.brand.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products.brand.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products_id_brand  = products[['Producto_ID', 'brand']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.merge(data, products_id_brand, on='Producto_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 5: Create clusters of Products (utility hypothesis) - ramdonly pick 30 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read files:\n",
    "product_clusters = pd.read_csv('input-data/producto_clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "product_clusters.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (product_clusters[\"cluster\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products_id_clusters = product_clusters[['Producto_ID', 'cluster']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products_id_clusters.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.merge(data, products_id_clusters, on='Producto_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 6: Create a category of Size of store based on Number of Agencies and Routes and Sales Channels that serve the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Determine pivot table\n",
    "Rutas_per_store = data.pivot_table(values=[\"Ruta_SAK\"], index=[\"Cliente_ID\"], aggfunc=pd.Series.nunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Rutas_per_store.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Agencies_per_store = data.pivot_table(values=[\"Agencia_ID\"], index=[\"Cliente_ID\"], aggfunc=pd.Series.nunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Agencies_per_store.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Canals_per_store = data.pivot_table(values=[\"Canal_ID\"], index=[\"Cliente_ID\"], aggfunc=pd.Series.nunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Canals_per_store.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't look that we can Bin on Canal_ID or Agencia_ID since they are not at least semi evenly ditributed, but it does look like Ruta_SAK is a good option based on the percentiles distribution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Rutas_per_store.rename(columns={'Ruta_SAK': 'Qty_Ruta_SAK'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mergin Routa_Sak's per client to data df\n",
    "data = pd.merge(data,Rutas_per_store,right_index=True, left_on='Cliente_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Binning:\n",
    "def binning(col, cut_points, labels=None):\n",
    "  #Define min and max values:\n",
    "  minval = col.min()\n",
    "  maxval = col.max()\n",
    "\n",
    "  #create list by adding min and max to cut_points\n",
    "  break_points = [minval] + cut_points + [maxval]\n",
    "\n",
    "  #if no labels provided, use default labels 0 ... (n-1)\n",
    "  if not labels:\n",
    "    labels = range(len(cut_points)+1)\n",
    "\n",
    "  #Binning using cut function of pandas\n",
    "  colBin = pd.cut(col,bins=break_points,labels=labels,include_lowest=True)\n",
    "  return colBin\n",
    "\n",
    "#Binning Qty_Ruta_SAK:\n",
    "cut_points = [2,4,10]\n",
    "labels = [\"low\",\"medium\",\"high\",\"very high\"]\n",
    "data[\"Qty_Ruta_SAK_Bin\"] = binning(data[\"Qty_Ruta_SAK\"], cut_points, labels)\n",
    "print (pd.value_counts(data[\"Qty_Ruta_SAK_Bin\"], sort=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We don't need Qty_Ruta_Sak anymore\n",
    "data.drop(['Qty_Ruta_SAK'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 7: Create a category of location based on zip code (embedded on town table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import os\n",
    "import time\n",
    "towns = pd.read_csv(\"input-data/town_state.csv\")\n",
    "L = lambda x: list(map(int, re.findall('\\d+', x)))[0]\n",
    "towns['ZipCode'] = towns.Town.apply(L) \n",
    "towns['ZipCode'] = np.uint16(towns['ZipCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zipcodes_df = towns[['Agencia_ID', 'ZipCode']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zipcodes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.merge(data, zipcodes_df, on='Agencia_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Numerical and One-Hot Coding of Categorical variables\n",
    "Since scikit-learn accepts only numerical variables, so i have to convert all categories of nominal variables into numeric types.\n",
    "\n",
    "Lets start with coding all low cardinality object/nominal categorical variables (brand, Qty_Ruta_SAK_Bin)  as numeric using ‘LabelEncoder’ from sklearn’s preprocessing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import library:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "var_mod = ['brand', 'Qty_Ruta_SAK_Bin']\n",
    "for i in var_mod:\n",
    "    data[i] = le.fit_transform(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One-Hot-Coding refers to creating dummy variables, one for each category of a categorical variable. For example, the 'cluster' variable has 30 categories. One hot coding will remove this variable and generate 30 new variables. Each will have binary numbers – 0 (if the category is not present) and 1(if category is present).\n",
    "Categorical variables are intentionally (for censorship) or implicitly encoded as numerical variables in order to be used as features in any given model.\n",
    "\n",
    "e.g. [house, car, tooth, car] becomes [0,1,2,1].\n",
    "\n",
    "This imparts an ordinal property to the variable, i.e. house < car < tooth.\n",
    "\n",
    "As this is ordinal characteristic is usually not desired, one hot encoding is necessary for the proper representation of the distinct elements of the variable.\n",
    "\n",
    "-- This can be done using ‘get_dummies’ function of Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One Hot Coding: you need python 3 and 128GB ram for this\n",
    "#tic()\n",
    "#data = pd.get_dummies(data, columns=['Canal_ID','brand','cluster','Qty_Ruta_SAK_Bin'])\n",
    "#tac()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the datatypes of columns now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\\. Exporting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Divide into test and train:\n",
    "import csv\n",
    "train = data.loc[data['source']==\"train\"]\n",
    "test = data.loc[data['source']==\"test\"]\n",
    "\n",
    "#Drop unnecessary columns:\n",
    "train.drop(['source','id','Venta_uni_hoy','Venta_hoy','Dev_uni_proxima','Dev_proxima'],axis=1,inplace=True)\n",
    "test.drop(['source','Venta_uni_hoy','Venta_hoy','Dev_uni_proxima', 'Dev_proxima','Demanda_uni_equil'],axis=1,inplace=True)\n",
    "\n",
    "#Export files as modified versions:\n",
    "tic()\n",
    "train.to_csv(\"./input-data/train_modified.csv\", index=False, quoting=csv.QUOTE_NONE)\n",
    "test.to_csv(\"./input-data/test_modified.csv\", index=False, quoting=csv.QUOTE_NONE)\n",
    "tac()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
